{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e4e4b8-2bd9-4db5-908e-649f615849b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, RobertaTokenizer\n",
    "\n",
    "# Load the pre-trained CodeT5 models and tokenizer\n",
    "small_model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "large_model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "\n",
    "def speculative_decoding(input_text, num_candidates=3, max_length=50):\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    \n",
    "    final_sequence = input_ids\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        candidate_outputs = small_model.generate(final_sequence, num_return_sequences=num_candidates, max_length=final_sequence.size(-1) + 1, do_sample=True)\n",
    "        candidate_tokens = [output[-1].unsqueeze(0) for output in candidate_outputs]\n",
    "        \n",
    "        decoder_input_ids = final_sequence\n",
    "\n",
    "        with torch.no_grad():\n",
    "            large_model_outputs = large_model(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            large_model_logits = large_model_outputs.logits[:, -1, :]\n",
    "        \n",
    "        predicted_token = torch.argmax(large_model_logits, dim=-1).item()\n",
    "\n",
    "        match_found = False\n",
    "        for candidate_token in candidate_tokens:\n",
    "            if candidate_token.item() == predicted_token:\n",
    "                final_sequence = torch.cat((final_sequence, candidate_token.unsqueeze(0)), dim=-1)\n",
    "                match_found = True\n",
    "                break\n",
    "        \n",
    "        if not match_found:\n",
    "            final_sequence = torch.cat((final_sequence, torch.tensor([[predicted_token]])), dim=-1)\n",
    "        \n",
    "        decoded_sequence = tokenizer.decode(final_sequence[0], skip_special_tokens=True)\n",
    "        \n",
    "        if decoded_sequence.endswith('</s>'):\n",
    "            break\n",
    "    \n",
    "    return decoded_sequence\n",
    "\n",
    "# Example usage\n",
    "# input_code = \"def add(a, b):\"\n",
    "# generated_code = speculative_decoding(input_code)\n",
    "# print(\"Generated Code:\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4e3334-e0d6-41ea-bc9e-7ac9087c4c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output:  ['public void serialize(LittleEndianOutput out) {out.writeShort(field_1']\n",
      "Correct answer: public void serialize(LittleEndianOutput out) {out.writeShort(field_1_vcenter);}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, RobertaTokenizer\n",
    "\n",
    "file_model = \"sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "file_assis_model = \"sh/saved_models/translate/cs-java/codet5_small_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "\n",
    "assistant_model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "model.load_state_dict(torch.load(file_model))\n",
    "assistant_model.load_state_dict(torch.load(file_assis_model))\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "input_code = \"public override void Serialize(ILittleEndianOutput out1){out1.WriteShort(field_1_vcenter);}\"\n",
    "\n",
    "inputs = tokenizer(input_code, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.3)\n",
    "print(\"Model output: \", tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "print(\"Correct answer: public void serialize(LittleEndianOutput out) {out.writeShort(field_1_vcenter);}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e54a048-3b7d-464c-9518-2cde61d0dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "import torch\n",
    "\n",
    "WORKDIR=\"/home/okozlova/diplom_oksana/CodeT5/CodeT5\"\n",
    "MODEL_TAG = 'codet5_large'\n",
    "MODEL_DIR = 'sh/saved_models'\n",
    "TASK = 'translate'\n",
    "SUB_TASK = 'cs-java'\n",
    "DATA_TAG = 'all'\n",
    "LR = 5e-5\n",
    "BS = 4\n",
    "SRC_LEN = 320\n",
    "TRG_LEN = 256\n",
    "PATIENCE = 5\n",
    "EPOCH = 100\n",
    "FULL_MODEL_TAG=f'{MODEL_TAG}_{DATA_TAG}_lr{LR}_bs{BS}_src{SRC_LEN}_trg{TRG_LEN}_pat{PATIENCE}_e{EPOCH}'\n",
    "OUTPUT_DIR=f'{MODEL_DIR}/{TASK}/{SUB_TASK}/{FULL_MODEL_TAG}'\n",
    "CACHE_DIR=f'{OUTPUT_DIR}/cache_data'\n",
    "RES_DIR=f'{OUTPUT_DIR}/prediction'\n",
    "LOG=f'{OUTPUT_DIR}/train.log'\n",
    "DATA_DIR = '/home/okozlova/diplom_oksana/CodeT5/CodeT5/data/translate/'\n",
    "args_dict = {\n",
    "    \"task\": 'translate',\n",
    "    \"sub_task\": SUB_TASK,\n",
    "    \"lang\": 'java', # 'c_sharp',\n",
    "    \"model_tag\": MODEL_TAG,\n",
    "    \"res_dir\": 'sh/results',\n",
    "    \"model_dir\": 'sh/saved_models',\n",
    "    \"summary_dir\": 'tensorboard',\n",
    "    \"data_num\": -1,\n",
    "    \"gpu\": 0,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_eval_bleu\": True,\n",
    "    \"do_test\": True,\n",
    "    \"model_type\": 'codet5',\n",
    "    \"num_train_epochs\": EPOCH, # 100\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"learning_rate\": LR,\n",
    "    \"patience\": PATIENCE, \n",
    "    \"tokenizer_name\": 'Salesforce/codet5-large',\n",
    "    \"model_name_or_path\": 'Salesforce/codet5-large',\n",
    "    'data_dir': WORKDIR + '/data',\n",
    "    'cache_path': CACHE_DIR,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'summary_dir': 'tensorboard',\n",
    "    'save_last_checkpoints': True,\n",
    "    'always_save_model': True,\n",
    "    'res_dir': RES_DIR,\n",
    "    'train_batch_size': BS,\n",
    "    'eval_batch_size': BS, \n",
    "    'max_source_length': SRC_LEN,\n",
    "    'max_target_length': TRG_LEN,\n",
    "    'seed': 1234,\n",
    "    'local_rank': 0,\n",
    "    'no_cuda': False,\n",
    "    'n_gpu': 1,\n",
    "    'device': torch.device(\"cuda\", 1),\n",
    "    'train_filename': f'{DATA_DIR}/train.java-cs.txt.cs,{DATA_DIR}/train.java-cs.txt.java',\n",
    "    'dev_filename': f'{DATA_DIR}/valid.java-cs.txt.cs,{DATA_DIR}/valid.java-cs.txt.java',\n",
    "    'test_filename': f'{DATA_DIR}/test.java-cs.txt.cs,{DATA_DIR}/test.java-cs.txt.java',\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'gradient_accumulation_steps': 1, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    'load_model_path': None,\n",
    "    'cpu_cont': 16,\n",
    "    'beam_size': 10\n",
    "    \n",
    "}\n",
    "\n",
    "args = DotMap(args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f02cb-ae0f-4b17-9f46-b25f96de75af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38c07681-08ce-4d24-be9e-5a053902dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval BLEU for test set: 100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [05:03<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7691698609766356, weighted ngram match: 0.7766263290183562, syntax_match: 0.8943953357156291, dataflow_match: 0.8863068465767117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval BLEU for test set: 100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [05:37<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7886146091898436, weighted ngram match: 0.7961580300663933, syntax_match: 0.8999905962008652, dataflow_match: 0.894552723638181\n",
      "With Speculative Decoding: {'inference_time': 303.8070125579834, 'bleu': 76.92, 'codebleu': 83.16245930718333}\n",
      "Without Speculative Decoding: {'inference_time': 337.8633964061737, 'bleu': 78.87, 'codebleu': 84.48289897738206}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from transformers import T5ForConditionalGeneration, RobertaTokenizer\n",
    "from tqdm import tqdm\n",
    "from evaluator.CodeBLEU import calc_code_bleu\n",
    "from evaluator.bleu import _bleu\n",
    "from evaluator import smooth_bleu\n",
    "from utils import read_examples, convert_examples_to_features, calc_stats\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_and_cache_gen_data(args, filename, tokenizer, split_tag, only_src=False, is_sample=False):\n",
    "    data_tag = '_all' if args.data_num == -1 else '_%d' % args.data_num\n",
    "    cache_fn = '{}/{}.pt'.format(args.cache_path, split_tag + ('_src' if only_src else '') + data_tag)\n",
    "\n",
    "    examples = read_examples(filename, args.data_num, args.task)\n",
    "\n",
    "    if is_sample:\n",
    "        examples = random.sample(examples, min(5000, len(examples)))\n",
    "    if split_tag == 'train':\n",
    "        calc_stats(examples, tokenizer, is_tokenize=True)\n",
    "    else:\n",
    "        calc_stats(examples)\n",
    "    if os.path.exists(cache_fn) and not is_sample:\n",
    "        logger.info(\"Load cache data from %s\", cache_fn)\n",
    "        data = torch.load(cache_fn)\n",
    "    else:\n",
    "        if is_sample:\n",
    "            logger.info(\"Sample 5k data for computing BLEU from %s\", filename)\n",
    "        else:\n",
    "            logger.info(\"Create cache data into %s\", cache_fn)\n",
    "        features = [convert_examples_to_features((example, idx, tokenizer, args, split_tag)) for idx, example in enumerate(tqdm(examples))]\n",
    "        all_source_ids = torch.tensor([f.source_ids for f in features], dtype=torch.long)\n",
    "        if split_tag == 'test' or only_src:\n",
    "            data = TensorDataset(all_source_ids)\n",
    "        else:\n",
    "            all_target_ids = torch.tensor([f.target_ids for f in features], dtype=torch.long)\n",
    "            data = TensorDataset(all_source_ids, all_target_ids)\n",
    "        if args.local_rank in [-1, 0] and not is_sample:\n",
    "            torch.save(data, cache_fn)\n",
    "    return examples, data\n",
    "\n",
    "def eval_bleu_epoch(args, eval_data, eval_examples, model, assistant_model, tokenizer, split_tag, criteria, speculative):\n",
    "    logger.info(\"***** Running BLEU evaluation on {} data *****\".format(split_tag))\n",
    "    logger.info(\"Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=1, num_workers=4, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    assistant_model.eval()\n",
    "\n",
    "    pred_ids = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Eval BLEU for {} set\".format(split_tag)):\n",
    "        source_ids = batch[0].to(args.device)\n",
    "        source_mask = source_ids.ne(tokenizer.pad_token_id).to(args.device)\n",
    "        with torch.no_grad():\n",
    "            if speculative:\n",
    "                preds = model.generate(\n",
    "                    source_ids,\n",
    "                    attention_mask=source_mask,\n",
    "                    do_sample=True,\n",
    "                    assistant_model=assistant_model,\n",
    "                    # temperature=0.3,\n",
    "                    num_beams=1,\n",
    "                    max_length=args.max_target_length\n",
    "                )\n",
    "            else:\n",
    "                preds = model.generate(\n",
    "                    source_ids,\n",
    "                    attention_mask=source_mask,\n",
    "                    use_cache=True,\n",
    "                    num_beams=1,\n",
    "                    max_length=args.max_target_length,\n",
    "                )\n",
    "            top_preds = list(preds.cpu().numpy())\n",
    "            pred_ids.extend(top_preds)\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    pred_nls = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False) for id in pred_ids]\n",
    "\n",
    "    output_fn = os.path.join(args.res_dir, \"test_{}.output\".format(criteria))\n",
    "    gold_fn = os.path.join(args.res_dir, \"test_{}.gold\".format(criteria))\n",
    "\n",
    "    with open(gold_fn, 'w') as f1:\n",
    "        for gold in eval_examples:\n",
    "            f1.write(gold.target.strip() + '\\n')\n",
    "\n",
    "    with open(output_fn, 'w') as f:\n",
    "        for pred_nl in pred_nls:\n",
    "            f.write(pred_nl.strip() + '\\n')\n",
    "\n",
    "    bleu = round(_bleu(gold_fn, output_fn), 2)\n",
    "    codebleu = calc_code_bleu.get_codebleu(gold_fn, output_fn, args.lang)\n",
    "\n",
    "    result = {\n",
    "        'inference_time': inference_time,\n",
    "        'bleu': bleu,\n",
    "        'codebleu': codebleu * 100\n",
    "    }\n",
    "\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(round(result[key], 4)))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Пути к моделям\n",
    "file_model = \"sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "file_assis_model = \"sh/saved_models/translate/cs-java/codet5_small_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "\n",
    "# Загрузка моделей и токенизатора\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "assistant_model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model.load_state_dict(torch.load(file_model, map_location=args.device))\n",
    "assistant_model.load_state_dict(torch.load(file_assis_model, map_location=args.device))\n",
    "model.to(args.device)\n",
    "assistant_model.to(args.device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "\n",
    "eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, tokenizer, \"test\")\n",
    "\n",
    "result_with_speculative = eval_bleu_epoch(args, eval_data, eval_examples, model, assistant_model, tokenizer, \"test\", \"speculative\", True)\n",
    "\n",
    "args.use_speculative_decoding = False\n",
    "result_without_speculative = eval_bleu_epoch(args, eval_data, eval_examples, model, assistant_model, tokenizer, \"test\", \"no_speculative\", False)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"With Speculative Decoding:\", result_with_speculative)\n",
    "print(\"Without Speculative Decoding:\", result_without_speculative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2640996-c7dc-4e04-8461-f4b2d25a3c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval BLEU for test set: 100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [05:36<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7507469060240761, weighted ngram match: 0.7570898394281561, syntax_match: 0.8884239232649991, dataflow_match: 0.8584457771114443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval BLEU for test set: 100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [10:30<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7743429767525913, weighted ngram match: 0.7851691343245184, syntax_match: 0.8981568553695694, dataflow_match: 0.8793103448275862\n",
      "With Speculative Decoding: {'inference_time': 336.6193685531616, 'bleu': 75.08, 'codebleu': 81.36766114571688}\n",
      "Without Speculative Decoding: {'inference_time': 630.3578195571899, 'bleu': 77.44, 'codebleu': 83.42448278185664}\n"
     ]
    }
   ],
   "source": [
    "# Пути к моделям\n",
    "file_model = \"sh/saved_models/translate/cs-java/codet5_large_all_lr5e-05_bs4_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "file_assis_model = \"sh/saved_models/translate/cs-java/codet5_small_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "\n",
    "# Загрузка моделей и токенизатора\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-large')\n",
    "assistant_model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model.load_state_dict(torch.load(file_model, map_location=args.device))\n",
    "assistant_model.load_state_dict(torch.load(file_assis_model, map_location=args.device))\n",
    "model.to(args.device)\n",
    "assistant_model.to(args.device)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "\n",
    "# Загрузка и кэширование данных\n",
    "eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, tokenizer, \"test\")\n",
    "\n",
    "# Выполнение оценки BLEU и CodeBLEU\n",
    "result_with_speculative = eval_bleu_epoch(args, eval_data, eval_examples, model, assistant_model, tokenizer, \"test\", \"speculative\", True)\n",
    "\n",
    "# Отключаем speculative decoding для повторной оценки\n",
    "args.use_speculative_decoding = False\n",
    "result_without_speculative = eval_bleu_epoch(args, eval_data, eval_examples, model, assistant_model, tokenizer, \"test\", \"no_speculative\", False)\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"With Speculative Decoding:\", result_with_speculative)\n",
    "print(\"Without Speculative Decoding:\", result_without_speculative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "575c5e51-bbac-4d6b-8d70-b2123e4044cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time with speculative decoding: 0.2066 seconds\n",
      "Inference time without speculative decoding: 0.1431 seconds\n",
      "Output with speculative decoding:  public void addAll(BlockList<T> src) {if (src.size ==\n",
      "Output without speculative decoding:  public void addAll(BlockList<T> src) {if (src.size ==\n",
      "Correct answer:  public void addAll(BlockList<T> src) {if (src.size == 0)return;int srcDirIdx = 0;for (; srcDirIdx < src.tailDirIdx; srcDirIdx++)addAll(src.directory[srcDirIdx], 0, BLOCK_SIZE);if (src.tailBlkIdx != 0)addAll(src.tailBlock, 0, src.tailBlkIdx);}\n",
      "BLEU score with speculative decoding:  3.3746151800550366\n",
      "BLEU score without speculative decoding:  3.3746151800550366\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import T5ForConditionalGeneration, RobertaTokenizer\n",
    "import sacrebleu\n",
    "\n",
    "# Пути к моделям\n",
    "file_model = \"sh/saved_models/translate/cs-java/codet5_large_all_lr5e-05_bs4_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "file_assis_model = \"sh/saved_models/translate/cs-java/codet5_small_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\"\n",
    "file_model = file_assis_model\n",
    "# Загрузка моделей и токенизатора\n",
    "assistant_model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small')\n",
    "model.load_state_dict(torch.load(file_model))\n",
    "assistant_model.load_state_dict(torch.load(file_assis_model))\n",
    "tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-small')\n",
    "\n",
    "input_code = \"public virtual void AddAll(NGit.Util.BlockList<T> src){if (src.size == 0){return;}int srcDirIdx = 0;for (; srcDirIdx < src.tailDirIdx; srcDirIdx++){AddAll(src.directory[srcDirIdx], 0, BLOCK_SIZE);}if (src.tailBlkIdx != 0){AddAll(src.tailBlock, 0, src.tailBlkIdx);}}\"\n",
    "inputs = tokenizer(input_code, return_tensors=\"pt\")\n",
    "\n",
    "# Correct output for BLEU calculation\n",
    "correct_answer = \"public void addAll(BlockList<T> src) {if (src.size == 0)return;int srcDirIdx = 0;for (; srcDirIdx < src.tailDirIdx; srcDirIdx++)addAll(src.directory[srcDirIdx], 0, BLOCK_SIZE);if (src.tailBlkIdx != 0)addAll(src.tailBlock, 0, src.tailBlkIdx);}\"\n",
    "references = [correct_answer]\n",
    "\n",
    "# Инференс с speculative decoding\n",
    "start_time = time.time()\n",
    "outputs_with_speculative = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.3)\n",
    "end_time = time.time()\n",
    "time_with_speculative = end_time - start_time\n",
    "output_with_speculative = tokenizer.batch_decode(outputs_with_speculative, skip_special_tokens=True)[0]\n",
    "\n",
    "# Инференс без speculative decoding\n",
    "start_time = time.time()\n",
    "outputs_without_speculative = model.generate(**inputs, do_sample=True, temperature=0.3)\n",
    "end_time = time.time()\n",
    "time_without_speculative = end_time - start_time\n",
    "output_without_speculative = tokenizer.batch_decode(outputs_without_speculative, skip_special_tokens=True)[0]\n",
    "\n",
    "# Расчет BLEU-метрики\n",
    "bleu_with_speculative = sacrebleu.corpus_bleu([output_with_speculative], [references])\n",
    "bleu_without_speculative = sacrebleu.corpus_bleu([output_without_speculative], [references])\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Inference time with speculative decoding: {:.4f} seconds\".format(time_with_speculative))\n",
    "print(\"Inference time without speculative decoding: {:.4f} seconds\".format(time_without_speculative))\n",
    "print(\"Output with speculative decoding: \", output_with_speculative)\n",
    "print(\"Output without speculative decoding: \", output_without_speculative)\n",
    "print(\"Correct answer: \", correct_answer)\n",
    "print(\"BLEU score with speculative decoding: \", bleu_with_speculative.score)\n",
    "print(\"BLEU score without speculative decoding: \", bleu_without_speculative.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df918c44-d18e-46fb-b4a5-4995affbfceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
