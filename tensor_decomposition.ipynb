{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9eb2e77-d003-42ac-ac7d-25b097f1cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import time\n",
    "from dotmap import DotMap\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import T5Config, T5ForConditionalGeneration, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb0a458-761d-4dab-9102-3cfef81a8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_or_load_gen_model\n",
    "from evaluator import smooth_bleu\n",
    "from evaluator.CodeBLEU import calc_code_bleu\n",
    "from evaluator.bleu import _bleu\n",
    "from utils import get_filenames, get_elapse_time, load_and_cache_gen_data\n",
    "from configs import set_seed, set_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33fe10b-c31d-45f4-a646-f30f9333b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def eval_ppl_epoch(args, eval_data, eval_examples, model, tokenizer):\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,\n",
    "                                 num_workers=4, pin_memory=True)\n",
    "    # Start evaluating model\n",
    "    logger.info(\"  \" + \"***** Running ppl evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, batch_num = 0, 0\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Eval ppl\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        source_ids, target_ids = batch\n",
    "        source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "        target_mask = target_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.model_type == 'roberta':\n",
    "                loss, _, _ = model(source_ids=source_ids, source_mask=source_mask,\n",
    "                                   target_ids=target_ids, target_mask=target_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids=source_ids, attention_mask=source_mask,\n",
    "                                labels=target_ids, decoder_attention_mask=target_mask)\n",
    "                loss = outputs.loss\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "        batch_num += 1\n",
    "    eval_loss = eval_loss / batch_num\n",
    "    eval_ppl = round(np.exp(eval_loss), 5)\n",
    "    return eval_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5190e6-a7ab-45b3-83be-8c7cd5ecc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, split_tag, criteria):\n",
    "    logger.info(\"  ***** Running bleu evaluation on {} data*****\".format(split_tag))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    if args.data_num == -1:\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,\n",
    "                                     num_workers=4, pin_memory=True)\n",
    "    else:\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    pred_ids = []\n",
    "    bleu, codebleu = 0.0, 0.0\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Eval bleu for {} set\".format(split_tag)):\n",
    "        source_ids = batch[0].to(args.device)\n",
    "        source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "        with torch.no_grad():\n",
    "            if args.model_type == 'roberta':\n",
    "                preds = model(source_ids=source_ids, source_mask=source_mask)\n",
    "\n",
    "                top_preds = [pred[0].cpu().numpy() for pred in preds]\n",
    "            else:\n",
    "                preds = model.generate(source_ids,\n",
    "                                       attention_mask=source_mask,\n",
    "                                       use_cache=True,\n",
    "                                       num_beams=args.beam_size,\n",
    "                                       early_stopping=args.task == 'summarize',\n",
    "                                       max_length=args.max_target_length)\n",
    "                top_preds = list(preds.cpu().numpy())\n",
    "            pred_ids.extend(top_preds)\n",
    "\n",
    "    pred_nls = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False) for id in pred_ids]\n",
    "\n",
    "    output_fn = os.path.join(args.res_dir, \"test_{}.output\".format(criteria))\n",
    "    gold_fn = os.path.join(args.res_dir, \"test_{}.gold\".format(criteria))\n",
    "    src_fn = os.path.join(args.res_dir, \"test_{}.src\".format(criteria))\n",
    "\n",
    "    if args.task in ['defect']:\n",
    "        target_dict = {0: 'false', 1: 'true'}\n",
    "        golds = [target_dict[ex.target] for ex in eval_examples]\n",
    "        eval_acc = np.mean([int(p == g) for p, g in zip(pred_nls, golds)])\n",
    "        result = {'em': eval_acc * 100, 'bleu': 0, 'codebleu': 0}\n",
    "\n",
    "        with open(output_fn, 'w') as f, open(gold_fn, 'w') as f1, open(src_fn, 'w') as f2:\n",
    "            for pred_nl, gold in zip(pred_nls, eval_examples):\n",
    "                f.write(pred_nl.strip() + '\\n')\n",
    "                f1.write(target_dict[gold.target] + '\\n')\n",
    "                f2.write(gold.source.strip() + '\\n')\n",
    "            logger.info(\"Save the predictions into %s\", output_fn)\n",
    "    else:\n",
    "        dev_accs, predictions = [], []\n",
    "        with open(output_fn, 'w') as f, open(gold_fn, 'w') as f1, open(src_fn, 'w') as f2:\n",
    "            for pred_nl, gold in zip(pred_nls, eval_examples):\n",
    "                dev_accs.append(pred_nl.strip() == gold.target.strip())\n",
    "                if args.task in ['summarize']:\n",
    "                    # for smooth-bleu4 evaluation\n",
    "                    predictions.append(str(gold.idx) + '\\t' + pred_nl)\n",
    "                    f.write(str(gold.idx) + '\\t' + pred_nl.strip() + '\\n')\n",
    "                    f1.write(str(gold.idx) + '\\t' + gold.target.strip() + '\\n')\n",
    "                    f2.write(str(gold.idx) + '\\t' + gold.source.strip() + '\\n')\n",
    "                else:\n",
    "                    f.write(pred_nl.strip() + '\\n')\n",
    "                    f1.write(gold.target.strip() + '\\n')\n",
    "                    f2.write(gold.source.strip() + '\\n')\n",
    "\n",
    "        if args.task == 'summarize':\n",
    "            (goldMap, predictionMap) = smooth_bleu.computeMaps(predictions, gold_fn)\n",
    "            bleu = round(smooth_bleu.bleuFromMaps(goldMap, predictionMap)[0], 2)\n",
    "        else:\n",
    "            bleu = round(_bleu(gold_fn, output_fn), 2)\n",
    "            # if args.task in ['concode', 'translate', 'refine']:\n",
    "            #    codebleu = calc_code_bleu.get_codebleu(gold_fn, output_fn, args.lang)\n",
    "\n",
    "        result = {'em': np.mean(dev_accs) * 100, 'bleu': bleu}\n",
    "        # result['codebleu'] = codebleu * 100\n",
    "\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(round(result[key], 4)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a08509-b153-42ac-83a5-da3760af4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dotmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2be3aa-4d1c-4a04-adaf-ab7c44f881e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f693b1-a5c0-429c-8ccb-0dad00b1bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1234\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "torch.manual_seed(seed_)\n",
    "torch.cuda.manual_seed_all(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c172e33-f549-4878-b15e-c3453f03f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR=\"/home/okozlova/diplom_oksana/CodeT5/CodeT5\"\n",
    "MODEL_TAG = 'codet5p-220m'\n",
    "MODEL_DIR = 'sh/saved_models'\n",
    "TASK = 'translate'\n",
    "SUB_TASK = 'cs-java'\n",
    "DATA_TAG = 'all'\n",
    "LR = 5e-5\n",
    "BS = 16\n",
    "SRC_LEN = 320\n",
    "TRG_LEN = 256\n",
    "PATIENCE = 5\n",
    "EPOCH = 100\n",
    "FULL_MODEL_TAG=f'{MODEL_TAG}_{DATA_TAG}_lr{LR}_bs{BS}_src{SRC_LEN}_trg{TRG_LEN}_pat{PATIENCE}_e{EPOCH}'\n",
    "OUTPUT_DIR=f'{MODEL_DIR}/{TASK}/{SUB_TASK}/{FULL_MODEL_TAG}'\n",
    "CACHE_DIR=f'{OUTPUT_DIR}/cache_data'\n",
    "RES_DIR=f'{OUTPUT_DIR}/prediction'\n",
    "LOG=f'{OUTPUT_DIR}/train.log'\n",
    "DATA_DIR = '/home/okozlova/diplom_oksana/CodeT5/CodeT5/data/translate/'\n",
    "args_dict = {\n",
    "    \"task\": 'translate',\n",
    "    \"sub_task\": SUB_TASK,\n",
    "    \"lang\": 'java', # 'c_sharp',\n",
    "    \"model_tag\": MODEL_TAG,\n",
    "    \"res_dir\": 'sh/results',\n",
    "    \"model_dir\": 'sh/saved_models',\n",
    "    \"summary_dir\": 'tensorboard',\n",
    "    \"data_num\": -1,\n",
    "    \"gpu\": 0,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_eval_bleu\": True,\n",
    "    \"do_test\": True,\n",
    "    \"model_type\": 'codet5',\n",
    "    \"num_train_epochs\": EPOCH, # 100\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"learning_rate\": LR,\n",
    "    \"patience\": PATIENCE, \n",
    "    \"tokenizer_name\": 'Salesforce/codet5p-220m',\n",
    "    \"model_name_or_path\": 'Salesforce/codet5p-220m',\n",
    "    'data_dir': WORKDIR + '/data',\n",
    "    'cache_path': CACHE_DIR,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'summary_dir': 'tensorboard',\n",
    "    'save_last_checkpoints': True,\n",
    "    'always_save_model': True,\n",
    "    'res_dir': RES_DIR,\n",
    "    'train_batch_size': BS,\n",
    "    'eval_batch_size': BS, \n",
    "    'max_source_length': SRC_LEN,\n",
    "    'max_target_length': TRG_LEN,\n",
    "    'seed': 1234,\n",
    "    'local_rank': 0,\n",
    "    'no_cuda': False,\n",
    "    'n_gpu': 1,\n",
    "    'device': torch.device(\"cuda\", 0),\n",
    "    'train_filename': f'{DATA_DIR}/train.java-cs.txt.cs,{DATA_DIR}/train.java-cs.txt.java',\n",
    "    'dev_filename': f'{DATA_DIR}/valid.java-cs.txt.cs,{DATA_DIR}/valid.java-cs.txt.java',\n",
    "    'test_filename': f'{DATA_DIR}/test.java-cs.txt.cs,{DATA_DIR}/test.java-cs.txt.java',\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'gradient_accumulation_steps': 1, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    'load_model_path': None,\n",
    "    'cpu_cont': 16,\n",
    "    'beam_size': 10\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5bdb14-f5c1-4f32-8dfb-ff407995cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DotMap(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565b4a05-71dc-46e5-bbe6-3ed0c10e63d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 12:59:16 - INFO - __main__ -   DotMap(task='translate', sub_task='cs-java', lang='java', model_tag='codet5p-220m', res_dir='sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/prediction', model_dir='sh/saved_models', summary_dir='tensorboard', data_num=-1, gpu=0, do_train=True, do_eval=True, do_eval_bleu=True, do_test=True, model_type='codet5', num_train_epochs=100, warmup_steps=1000, learning_rate=5e-05, patience=5, tokenizer_name='Salesforce/codet5p-220m', model_name_or_path='Salesforce/codet5p-220m', data_dir='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data', cache_path='sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data', output_dir='sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100', save_last_checkpoints=True, always_save_model=True, train_batch_size=16, eval_batch_size=16, max_source_length=320, max_target_length=256, seed=1234, local_rank=0, no_cuda=False, n_gpu=1, device=device(type='cuda', index=0), train_filename='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//train.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//train.java-cs.txt.java', dev_filename='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java', test_filename='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//test.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//test.java-cs.txt.java', weight_decay=0.0, adam_epsilon=1e-08, gradient_accumulation_steps=1, load_model_path=None, cpu_cont=16, beam_size=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.model_type:  codet5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 12:59:18 - INFO - models -   Finish loading model [223M] from Salesforce/codet5p-220m\n",
      "05/24/2024 12:59:18 - INFO - __main__ -   Reload model from sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-decompose/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-decompose/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 12:59:24 - INFO - utils -   Read 10295 examples, avg src len: 15, avg trg len: 13, max src len: 118, max trg len: 136\n",
      "05/24/2024 12:59:24 - INFO - utils -   [TOKENIZE] avg src len: 56, avg trg len: 45, max src len: 404, max trg len: 391\n",
      "05/24/2024 12:59:24 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/train_all.pt\n",
      "/home/mzhelezin/venvs/otorch/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "05/24/2024 12:59:24 - INFO - __main__ -   ***** Running training *****\n",
      "05/24/2024 12:59:24 - INFO - __main__ -     Num examples = 10295\n",
      "05/24/2024 12:59:24 - INFO - __main__ -     Batch size = 16\n",
      "05/24/2024 12:59:24 - INFO - __main__ -     Batch num = 644\n",
      "05/24/2024 12:59:24 - INFO - __main__ -     Num epoch = 100\n",
      "[0] Train loss 0.021: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:24<00:00,  1.98it/s]\n",
      "05/24/2024 13:04:49 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:04:49 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/dev_all.pt\n",
      "05/24/2024 13:04:49 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:04:49 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:04:49 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.05it/s]\n",
      "05/24/2024 13:04:54 - INFO - __main__ -     epoch = 0\n",
      "05/24/2024 13:04:54 - INFO - __main__ -     eval_ppl = 1.03627\n",
      "05/24/2024 13:04:54 - INFO - __main__ -     global_step = 644\n",
      "05/24/2024 13:04:54 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:04:55 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:04:55 - INFO - __main__ -     Best ppl:1.03627\n",
      "05/24/2024 13:04:55 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:04:56 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/24/2024 13:04:56 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:04:56 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:04:56 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 279.73it/s]\n",
      "05/24/2024 13:04:58 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:04:58 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:04:58 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [02:51<00:00,  5.35s/it]\n",
      "05/24/2024 13:07:55 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 13:07:55 - INFO - __main__ -     bleu = 76.79\n",
      "05/24/2024 13:07:55 - INFO - __main__ -     codebleu = 83.4663\n",
      "05/24/2024 13:07:55 - INFO - __main__ -     em = 62.3246\n",
      "05/24/2024 13:07:55 - INFO - __main__ -     [0] Best bleu+em: 139.11 (bleu: 76.79, em: 62.32)\n",
      "05/24/2024 13:07:55 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7678423301186774, weighted ngram match: 0.7746447792035402, syntax_match: 0.8942449156209433, dataflow_match: 0.9019180470793374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 13:07:55 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/24/2024 13:07:55 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[1] Train loss 0.006: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:26<00:00,  1.97it/s]\n",
      "05/24/2024 13:13:23 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:13:23 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:13:23 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 13:13:28 - INFO - __main__ -     epoch = 1\n",
      "05/24/2024 13:13:28 - INFO - __main__ -     eval_ppl = 1.03558\n",
      "05/24/2024 13:13:28 - INFO - __main__ -     global_step = 1288\n",
      "05/24/2024 13:13:28 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:13:29 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:13:29 - INFO - __main__ -     Best ppl:1.03558\n",
      "05/24/2024 13:13:29 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:13:30 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/24/2024 13:13:30 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:13:30 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:13:30 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 278.22it/s]\n",
      "05/24/2024 13:13:32 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:13:32 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:13:32 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [03:07<00:00,  5.85s/it]\n",
      "05/24/2024 13:16:45 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 13:16:45 - INFO - __main__ -     bleu = 77.31\n",
      "05/24/2024 13:16:45 - INFO - __main__ -     codebleu = 83.7678\n",
      "05/24/2024 13:16:45 - INFO - __main__ -     em = 63.5271\n",
      "05/24/2024 13:16:45 - INFO - __main__ -     [1] Best bleu+em: 140.84 (bleu: 77.31, em: 63.53)\n",
      "05/24/2024 13:16:45 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7730145884303201, weighted ngram match: 0.7956025378623895, syntax_match: 0.8984855041107745, dataflow_match: 0.8836094158674804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 13:16:46 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/24/2024 13:16:46 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[2] Train loss 0.004: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:26<00:00,  1.97it/s]\n",
      "05/24/2024 13:22:13 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:22:13 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:22:13 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 13:22:18 - INFO - __main__ -     epoch = 2\n",
      "05/24/2024 13:22:18 - INFO - __main__ -     eval_ppl = 1.03604\n",
      "05/24/2024 13:22:18 - INFO - __main__ -     global_step = 1932\n",
      "05/24/2024 13:22:18 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:22:19 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:22:19 - INFO - __main__ -   Ppl does not decrease for 1 epochs\n",
      "05/24/2024 13:22:19 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:22:19 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:22:19 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 298.43it/s]\n",
      "05/24/2024 13:22:21 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:22:21 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:22:21 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [03:03<00:00,  5.74s/it]\n",
      "05/24/2024 13:25:30 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 13:25:30 - INFO - __main__ -     bleu = 79.44\n",
      "05/24/2024 13:25:30 - INFO - __main__ -     codebleu = 84.8582\n",
      "05/24/2024 13:25:30 - INFO - __main__ -     em = 65.5311\n",
      "05/24/2024 13:25:30 - INFO - __main__ -     [2] Best bleu+em: 144.97 (bleu: 79.44, em: 65.53)\n",
      "05/24/2024 13:25:30 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7943773747408851, weighted ngram match: 0.7992650595224503, syntax_match: 0.9016010385114669, dataflow_match: 0.8990845684394071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 13:25:31 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/24/2024 13:25:31 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[3] Train loss 0.004: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:26<00:00,  1.97it/s]\n",
      "05/24/2024 13:30:58 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:30:58 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:30:58 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 13:31:03 - INFO - __main__ -     epoch = 3\n",
      "05/24/2024 13:31:03 - INFO - __main__ -     eval_ppl = 1.03743\n",
      "05/24/2024 13:31:03 - INFO - __main__ -     global_step = 2576\n",
      "05/24/2024 13:31:03 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:31:04 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:31:04 - INFO - __main__ -   Ppl does not decrease for 2 epochs\n",
      "05/24/2024 13:31:04 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:31:04 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:31:04 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 297.10it/s]\n",
      "05/24/2024 13:31:06 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:31:06 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:31:06 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [03:08<00:00,  5.88s/it]\n",
      "05/24/2024 13:34:20 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 13:34:20 - INFO - __main__ -     bleu = 77.35\n",
      "05/24/2024 13:34:20 - INFO - __main__ -     codebleu = 83.0638\n",
      "05/24/2024 13:34:20 - INFO - __main__ -     em = 63.7275\n",
      "05/24/2024 13:34:20 - INFO - __main__ -   Bleu does not increase for 1 epochs\n",
      "05/24/2024 13:34:20 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7734674821393224, weighted ngram match: 0.784006890371543, syntax_match: 0.8888792730419731, dataflow_match: 0.8761987794245859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[4] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:26<00:00,  1.97it/s]\n",
      "05/24/2024 13:39:47 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:39:47 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:39:47 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 13:39:52 - INFO - __main__ -     epoch = 4\n",
      "05/24/2024 13:39:52 - INFO - __main__ -     eval_ppl = 1.0376\n",
      "05/24/2024 13:39:52 - INFO - __main__ -     global_step = 3220\n",
      "05/24/2024 13:39:52 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:39:53 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:39:53 - INFO - __main__ -   Ppl does not decrease for 3 epochs\n",
      "05/24/2024 13:39:53 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:39:54 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:39:54 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 300.45it/s]\n",
      "05/24/2024 13:39:55 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:39:55 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:39:55 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [02:59<00:00,  5.62s/it]\n",
      "05/24/2024 13:43:01 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 13:43:01 - INFO - __main__ -     bleu = 77.3\n",
      "05/24/2024 13:43:01 - INFO - __main__ -     codebleu = 84.2559\n",
      "05/24/2024 13:43:01 - INFO - __main__ -     em = 64.1283\n",
      "05/24/2024 13:43:01 - INFO - __main__ -   Bleu does not increase for 2 epochs\n",
      "05/24/2024 13:43:01 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7729337580169928, weighted ngram match: 0.7832488181173539, syntax_match: 0.9034184335785375, dataflow_match: 0.9106364428945074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[5] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/24/2024 13:48:28 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:48:28 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:48:28 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 13:48:34 - INFO - __main__ -     epoch = 5\n",
      "05/24/2024 13:48:34 - INFO - __main__ -     eval_ppl = 1.03724\n",
      "05/24/2024 13:48:34 - INFO - __main__ -     global_step = 3864\n",
      "05/24/2024 13:48:34 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:48:34 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:48:34 - INFO - __main__ -   Ppl does not decrease for 4 epochs\n",
      "05/24/2024 13:48:34 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:48:35 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:48:35 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 301.28it/s]\n",
      "05/24/2024 13:48:36 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:48:36 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:48:36 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [03:06<00:00,  5.84s/it]\n",
      "05/24/2024 13:51:49 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 13:51:49 - INFO - __main__ -     bleu = 78.47\n",
      "05/24/2024 13:51:49 - INFO - __main__ -     codebleu = 84.9187\n",
      "05/24/2024 13:51:49 - INFO - __main__ -     em = 65.1303\n",
      "05/24/2024 13:51:49 - INFO - __main__ -   Bleu does not increase for 3 epochs\n",
      "05/24/2024 13:51:49 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7846679724055893, weighted ngram match: 0.8036756559589567, syntax_match: 0.9012548680225011, dataflow_match: 0.9071490845684395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[6] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/24/2024 13:57:16 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 13:57:16 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:57:16 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 13:57:22 - INFO - __main__ -     epoch = 6\n",
      "05/24/2024 13:57:22 - INFO - __main__ -     eval_ppl = 1.03795\n",
      "05/24/2024 13:57:22 - INFO - __main__ -     global_step = 4508\n",
      "05/24/2024 13:57:22 - INFO - __main__ -     ********************\n",
      "05/24/2024 13:57:22 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 13:57:22 - INFO - __main__ -   Ppl does not decrease for 5 epochs\n",
      "05/24/2024 13:57:22 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 13:57:23 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 13:57:23 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 301.75it/s]\n",
      "05/24/2024 13:57:24 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 13:57:24 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 13:57:24 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [02:59<00:00,  5.62s/it]\n",
      "05/24/2024 14:00:30 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 14:00:30 - INFO - __main__ -     bleu = 78.42\n",
      "05/24/2024 14:00:30 - INFO - __main__ -     codebleu = 84.6371\n",
      "05/24/2024 14:00:30 - INFO - __main__ -     em = 65.1303\n",
      "05/24/2024 14:00:30 - INFO - __main__ -   Bleu does not increase for 4 epochs\n",
      "05/24/2024 14:00:30 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7841568447350579, weighted ngram match: 0.7990756290428533, syntax_match: 0.9018606663781913, dataflow_match: 0.9003923278116827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/24/2024 14:05:57 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 14:05:57 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 14:05:57 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/24/2024 14:06:02 - INFO - __main__ -     epoch = 7\n",
      "05/24/2024 14:06:02 - INFO - __main__ -     eval_ppl = 1.03715\n",
      "05/24/2024 14:06:02 - INFO - __main__ -     global_step = 5152\n",
      "05/24/2024 14:06:02 - INFO - __main__ -     ********************\n",
      "05/24/2024 14:06:03 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 14:06:03 - INFO - __main__ -   Ppl does not decrease for 6 epochs\n",
      "05/24/2024 14:06:03 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 14:06:04 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 14:06:04 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 301.52it/s]\n",
      "05/24/2024 14:06:05 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 14:06:05 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 14:06:05 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [02:52<00:00,  5.40s/it]\n",
      "05/24/2024 14:09:04 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 14:09:04 - INFO - __main__ -     bleu = 79.29\n",
      "05/24/2024 14:09:04 - INFO - __main__ -     codebleu = 84.9692\n",
      "05/24/2024 14:09:04 - INFO - __main__ -     em = 63.5271\n",
      "05/24/2024 14:09:04 - INFO - __main__ -   Bleu does not increase for 5 epochs\n",
      "05/24/2024 14:09:04 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7928441182172812, weighted ngram match: 0.7992552257674952, syntax_match: 0.8999567286888793, dataflow_match: 0.9067131647776809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[8] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████| 644/644 [05:26<00:00,  1.97it/s]\n",
      "05/24/2024 14:14:31 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/24/2024 14:14:31 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 14:14:31 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/24/2024 14:14:36 - INFO - __main__ -     epoch = 8\n",
      "05/24/2024 14:14:36 - INFO - __main__ -     eval_ppl = 1.03815\n",
      "05/24/2024 14:14:36 - INFO - __main__ -     global_step = 5796\n",
      "05/24/2024 14:14:36 - INFO - __main__ -     ********************\n",
      "05/24/2024 14:14:37 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/24/2024 14:14:37 - INFO - __main__ -   Ppl does not decrease for 7 epochs\n",
      "05/24/2024 14:14:37 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/24/2024 14:14:37 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/24/2024 14:14:37 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 300.97it/s]\n",
      "05/24/2024 14:14:39 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/24/2024 14:14:39 - INFO - __main__ -     Num examples = 499\n",
      "05/24/2024 14:14:39 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|███████████████████████████████████████████████████████████████████████████████████| 32/32 [03:11<00:00,  5.99s/it]\n",
      "05/24/2024 14:17:57 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     bleu = 78.29\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     codebleu = 84.1558\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     em = 64.1283\n",
      "05/24/2024 14:17:57 - INFO - __main__ -   Bleu does not increase for 6 epochs\n",
      "05/24/2024 14:17:57 - INFO - __main__ -   [8] Early stop as not_bleu_em_inc_cnt=6, and not_loss_dec_cnt=7\n",
      "\n",
      "05/24/2024 14:17:57 - INFO - __main__ -   Finish training and take 1h18m\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     ***** Testing *****\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     Batch size = 16\n",
      "05/24/2024 14:17:57 - INFO - __main__ -   Reload model from sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7828271110822578, weighted ngram match: 0.798854407122174, syntax_match: 0.8906966681090437, dataflow_match: 0.8938535309503052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/24/2024 14:17:57 - INFO - utils -   Read 1000 examples, avg src len: 14, avg trg len: 13, max src len: 94, max trg len: 98\n",
      "05/24/2024 14:17:57 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/test_src_all.pt\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     Num examples = 1000\n",
      "05/24/2024 14:17:57 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for test set: 100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [05:09<00:00,  4.91s/it]\n",
      "05/24/2024 14:23:16 - INFO - __main__ -   ***** Eval results *****\n",
      "05/24/2024 14:23:16 - INFO - __main__ -     bleu = 79.7\n",
      "05/24/2024 14:23:16 - INFO - __main__ -     codebleu = 85.1677\n",
      "05/24/2024 14:23:16 - INFO - __main__ -     em = 65.3\n",
      "05/24/2024 14:23:16 - INFO - __main__ -   [best-bleu] bleu-4: 79.70, em: 65.3000, codebleu: 85.1677\n",
      "\n",
      "05/24/2024 14:23:16 - INFO - __main__ -   Finish and take 1h24m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7969467608100157, weighted ngram match: 0.8056190591647376, syntax_match: 0.9067143125822832, dataflow_match: 0.8974262868565717\n"
     ]
    }
   ],
   "source": [
    "logger.info(args)\n",
    "t0 = time.time()\n",
    "\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "file = 'sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-decompose/pytorch_model.bin' #os.path.join(args.output_dir, 'checkpoint-{}/pytorch_model.bin'.format(criteria))\n",
    "print(file)\n",
    "logger.info(\"Reload model from {}\".format(file))\n",
    "model.load_state_dict(torch.load(file))\n",
    "model.to(args.device)\n",
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "pool = multiprocessing.Pool(args.cpu_cont)\n",
    "fa = open(os.path.join(args.output_dir, 'summary.log'), 'a+')\n",
    "\n",
    "if args.do_train:\n",
    "    if args.local_rank in [-1, 0] and args.data_num == -1:\n",
    "        summary_fn = '{}/{}'.format(args.summary_dir, '/'.join(args.output_dir.split('/')[1:]))\n",
    "        tb_writer = SummaryWriter(summary_fn)\n",
    "\n",
    "    # Prepare training data loader\n",
    "    train_examples, train_data = load_and_cache_gen_data(args, args.train_filename, pool, tokenizer, 'train')\n",
    "    train_sampler = RandomSampler(train_data) \n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                                  num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    num_train_optimization_steps = args.num_train_epochs * len(train_dataloader)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "    # Start training\n",
    "    train_example_num = len(train_data)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", train_example_num)\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Batch num = %d\", math.ceil(train_example_num / args.train_batch_size))\n",
    "    logger.info(\"  Num epoch = %d\", args.num_train_epochs)\n",
    "\n",
    "    dev_dataset = {}\n",
    "    global_step, best_bleu_em, best_ppl = 0, -1, 1e6\n",
    "    not_loss_dec_cnt, not_bleu_em_inc_cnt = 0, 0 if args.do_eval_bleu else 1e6\n",
    "\n",
    "    for cur_epoch in range(0, int(args.num_train_epochs)):\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "        nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(bar):\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            source_ids, target_ids = batch\n",
    "            source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "            target_mask = target_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "            if args.model_type == 'roberta':\n",
    "                loss, _, _ = model(source_ids=source_ids, source_mask=source_mask,\n",
    "                                   target_ids=target_ids, target_mask=target_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids=source_ids, attention_mask=source_mask,\n",
    "                                labels=target_ids, decoder_attention_mask=target_mask)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_examples += source_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            loss.backward()\n",
    "\n",
    "            if nb_tr_steps % args.gradient_accumulation_steps == 0:\n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "                train_loss = round(tr_loss * args.gradient_accumulation_steps / (nb_tr_steps + 1), 4)\n",
    "                bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "\n",
    "        if args.do_eval:\n",
    "            # Eval model with dev dataset\n",
    "            if 'dev_loss' in dev_dataset:\n",
    "                eval_examples, eval_data = dev_dataset['dev_loss']\n",
    "            else:\n",
    "                eval_examples, eval_data = load_and_cache_gen_data(args, args.dev_filename, pool, tokenizer, 'dev')\n",
    "                dev_dataset['dev_loss'] = eval_examples, eval_data\n",
    "\n",
    "            eval_ppl = eval_ppl_epoch(args, eval_data, eval_examples, model, tokenizer)\n",
    "            result = {'epoch': cur_epoch, 'global_step': global_step, 'eval_ppl': eval_ppl}\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            logger.info(\"  \" + \"*\" * 20)\n",
    "            if args.data_num == -1:\n",
    "                tb_writer.add_scalar('dev_ppl', eval_ppl, cur_epoch)\n",
    "\n",
    "            # save last checkpoint\n",
    "            if args.save_last_checkpoints:\n",
    "                last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "                if not os.path.exists(last_output_dir):\n",
    "                    os.makedirs(last_output_dir)\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                logger.info(\"Save the last model into %s\", output_model_file)\n",
    "\n",
    "            if eval_ppl < best_ppl:\n",
    "                not_loss_dec_cnt = 0\n",
    "                logger.info(\"  Best ppl:%s\", eval_ppl)\n",
    "                logger.info(\"  \" + \"*\" * 20)\n",
    "                fa.write(\"[%d] Best ppl changed into %.4f\\n\" % (cur_epoch, eval_ppl))\n",
    "                best_ppl = eval_ppl\n",
    "\n",
    "                # Save best checkpoint for best ppl\n",
    "                output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                if args.always_save_model:\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "                    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                    logger.info(\"Save the best ppl model into %s\", output_model_file)\n",
    "            else:\n",
    "                not_loss_dec_cnt += 1\n",
    "                logger.info(\"Ppl does not decrease for %d epochs\", not_loss_dec_cnt)\n",
    "                if all([x > args.patience for x in [not_bleu_em_inc_cnt, not_loss_dec_cnt]]):\n",
    "                    early_stop_str = \"[%d] Early stop as not_bleu_em_inc_cnt=%d, and not_loss_dec_cnt=%d\\n\" % (\n",
    "                        cur_epoch, not_bleu_em_inc_cnt, not_loss_dec_cnt)\n",
    "                    logger.info(early_stop_str)\n",
    "                    fa.write(early_stop_str)\n",
    "                    break\n",
    "            logger.info(\"***** CUDA.empty_cache() *****\")\n",
    "            torch.cuda.empty_cache()\n",
    "            if args.do_eval_bleu:\n",
    "                eval_examples, eval_data = load_and_cache_gen_data(args, args.dev_filename, pool, tokenizer, 'dev',\n",
    "                                                                   only_src=True, is_sample=True)\n",
    "\n",
    "                result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'dev', 'e%d' % cur_epoch)\n",
    "                dev_bleu, dev_em = result['bleu'], result['em']\n",
    "                if args.task in ['summarize']:\n",
    "                    dev_bleu_em = dev_bleu\n",
    "                elif args.task in ['defect']:\n",
    "                    dev_bleu_em = dev_em\n",
    "                else:\n",
    "                    dev_bleu_em = dev_bleu + dev_em\n",
    "                if args.data_num == -1:\n",
    "                    tb_writer.add_scalar('dev_bleu_em', dev_bleu_em, cur_epoch)\n",
    "                    # tb_writer.add_scalar('dev_em', dev_em, cur_epoch)\n",
    "                if dev_bleu_em > best_bleu_em:\n",
    "                    not_bleu_em_inc_cnt = 0\n",
    "                    logger.info(\"  [%d] Best bleu+em: %.2f (bleu: %.2f, em: %.2f)\",\n",
    "                                cur_epoch, dev_bleu_em, dev_bleu, dev_em)\n",
    "                    logger.info(\"  \" + \"*\" * 20)\n",
    "                    best_bleu_em = dev_bleu_em\n",
    "                    fa.write(\"[%d] Best bleu+em changed into %.2f (bleu: %.2f, em: %.2f)\\n\" % (\n",
    "                        cur_epoch, best_bleu_em, dev_bleu, dev_em))\n",
    "                    # Save best checkpoint for best bleu\n",
    "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    if args.data_num == -1 or args.always_save_model:\n",
    "                        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "                        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                        logger.info(\"Save the best bleu model into %s\", output_model_file)\n",
    "                else:\n",
    "                    not_bleu_em_inc_cnt += 1\n",
    "                    logger.info(\"Bleu does not increase for %d epochs\", not_bleu_em_inc_cnt)\n",
    "                    fa.write(\n",
    "                        \"[%d] Best bleu+em (%.2f) does not drop changed for %d epochs, cur bleu+em: %.2f (bleu: %.2f, em: %.2f)\\n\" % (\n",
    "                            cur_epoch, best_bleu_em, not_bleu_em_inc_cnt, dev_bleu_em, dev_bleu, dev_em))\n",
    "                    if all([x > args.patience for x in [not_bleu_em_inc_cnt, not_loss_dec_cnt]]):\n",
    "                        stop_early_str = \"[%d] Early stop as not_bleu_em_inc_cnt=%d, and not_loss_dec_cnt=%d\\n\" % (\n",
    "                            cur_epoch, not_bleu_em_inc_cnt, not_loss_dec_cnt)\n",
    "                        logger.info(stop_early_str)\n",
    "                        fa.write(stop_early_str)\n",
    "                        break\n",
    "        logger.info(\"***** CUDA.empty_cache() *****\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if args.local_rank in [-1, 0] and args.data_num == -1:\n",
    "        tb_writer.close()\n",
    "    logger.info(\"Finish training and take %s\", get_elapse_time(t0))\n",
    "\n",
    "if args.do_test:\n",
    "    logger.info(\"  \" + \"***** Testing *****\")\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    for criteria in ['best-bleu']:\n",
    "        file = os.path.join(args.output_dir, 'checkpoint-{}/pytorch_model.bin'.format(criteria))\n",
    "        logger.info(\"Reload model from {}\".format(file))\n",
    "        model.load_state_dict(torch.load(file))\n",
    "        eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, pool, tokenizer, 'test',\n",
    "                                                           only_src=True, is_sample=False)\n",
    "        result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'test', criteria)\n",
    "        test_bleu, test_em = result['bleu'], result['em']\n",
    "        test_codebleu = result['codebleu'] if 'codebleu' in result else 0\n",
    "        result_str = \"[%s] bleu-4: %.2f, em: %.4f, codebleu: %.4f\\n\" % (criteria, test_bleu, test_em, test_codebleu)\n",
    "        logger.info(result_str)\n",
    "        fa.write(result_str)\n",
    "        if args.res_fn:\n",
    "            with open(args.res_fn, 'a+') as f:\n",
    "                f.write('[Time: {}] {}\\n'.format(get_elapse_time(t0), file))\n",
    "                f.write(result_str)\n",
    "logger.info(\"Finish and take {}\".format(get_elapse_time(t0)))\n",
    "fa.write(\"Finish and take {}\".format(get_elapse_time(t0)))\n",
    "fa.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "285c215c-a3f3-4ab1-a7e5-58a9894c5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de90b3f8-2281-40ed-ae6c-8ef2c8fe65e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222882048"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e12647e-4b42-46fd-b27a-f640bb2537d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2024 14:36:45 - INFO - __main__ -     ***** Testing *****\n",
      "05/14/2024 14:36:45 - INFO - __main__ -     Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.model_type:  codet5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2024 14:36:47 - INFO - models -   Finish loading model [738M] from Salesforce/codet5-large\n",
      "05/14/2024 14:36:49 - INFO - __main__ -   Reload model from sh/saved_models/translate/cs-java/codet5_large_all_lr5e-05_bs4_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh/saved_models/translate/cs-java/codet5_large_all_lr5e-05_bs4_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/14/2024 14:36:50 - INFO - utils -   Read 1000 examples, avg src len: 14, avg trg len: 13, max src len: 94, max trg len: 98\n",
      "05/14/2024 14:36:50 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5_large_all_lr5e-05_bs4_src320_trg256_pat5_e100/cache_data/test_src_all.pt\n",
      "05/14/2024 14:36:50 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "05/14/2024 14:36:50 - INFO - __main__ -     Num examples = 1000\n",
      "05/14/2024 14:36:50 - INFO - __main__ -     Batch size = 4\n",
      "Eval bleu for test set: 100%|█████████████████████████████████████████████████████████████████████████████████████| 250/250 [09:47<00:00,  2.35s/it]\n",
      "05/14/2024 14:46:44 - INFO - __main__ -   ***** Eval results *****\n",
      "05/14/2024 14:46:44 - INFO - __main__ -     bleu = 78.06\n",
      "05/14/2024 14:46:44 - INFO - __main__ -     codebleu = 83.9113\n",
      "05/14/2024 14:46:44 - INFO - __main__ -     em = 66.2\n",
      "05/14/2024 14:46:44 - INFO - __main__ -   [best-bleu] bleu-4: 78.06, em: 66.2000, codebleu: 83.9113\n",
      "\n",
      "05/14/2024 14:46:44 - INFO - __main__ -   Finish and take 9m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.780570768115452, weighted ngram match: 0.7873614145521678, syntax_match: 0.9012130900883957, dataflow_match: 0.8873063468265867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-16:\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "logger.info(\"  \" + \"***** Testing *****\")\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "# args.model_name_or_path = Salesforce/codet5-base'\n",
    "pool = multiprocessing.Pool(args.cpu_cont)\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "model.to(args.device)\n",
    "\n",
    "for criteria in ['best-bleu']:\n",
    "    file = 'sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin' #os.path.join(args.output_dir, 'checkpoint-{}/pytorch_model.bin'.format(criteria))\n",
    "    print(file)\n",
    "    logger.info(\"Reload model from {}\".format(file))\n",
    "    model.load_state_dict(torch.load(file))\n",
    "    eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, pool, tokenizer, 'test',\n",
    "                                                       only_src=True, is_sample=False)\n",
    "    result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'test', criteria)\n",
    "    test_bleu, test_em = result['bleu'], result['em']\n",
    "    test_codebleu = result['codebleu'] if 'codebleu' in result else 0\n",
    "    result_str = \"[%s] bleu-4: %.2f, em: %.4f, codebleu: %.4f\\n\" % (criteria, test_bleu, test_em, test_codebleu)\n",
    "    logger.info(result_str)\n",
    "    # fa.write(result_str)\n",
    "    # if args.res_fn:\n",
    "    #     with open(args.res_fn, 'a+') as f:\n",
    "    #         f.write('[Time: {}] {}\\n'.format(get_elapse_time(t0), file))\n",
    "    #         f.write(result_str)\n",
    "logger.info(\"Finish and take {}\".format(get_elapse_time(t0)))\n",
    "# fa.write(\"Finish and take {}\".format(get_elapse_time(t0)))\n",
    "# fa.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2896ca2-2d31-4f7a-9fc2-f3d2a86a7d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorly as tl\n",
    "from tensorly.tenalg import svd_interface\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "decomposition_info = []\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def decompose_weights(weight_matrix, rank):\n",
    "    U, S, V = svd_interface(weight_matrix.cpu().detach().numpy(), n_eigenvecs=rank)\n",
    "    return torch.tensor(U).to(weight_matrix.device), torch.tensor(S).to(weight_matrix.device), torch.tensor(V).to(weight_matrix.device)\n",
    "\n",
    "def reconstruct_weights(U, S, V):\n",
    "    return torch.matmul(U, torch.matmul(torch.diag(S), V))\n",
    "\n",
    "def decompose_and_replace_linear_layer(model, layer_name, rank):\n",
    "    original_layer = dict(model.named_modules())[layer_name]\n",
    "    original_weight = original_layer.weight.detach().clone()\n",
    "    U, S, V = decompose_weights(original_layer.weight, rank)\n",
    "\n",
    "    layer1 = nn.Linear(U.size(0), U.size(1), bias=False)\n",
    "    layer2 = nn.Linear(S.size(0), S.size(0), bias=False)\n",
    "    layer3 = nn.Linear(V.size(0), V.size(1), bias=True)\n",
    "\n",
    "    layer1.weight.data = U\n",
    "    layer2.weight.data = torch.diag(S)\n",
    "    layer3.weight.data = V\n",
    "\n",
    "    setattr(model, layer_name, nn.Sequential(layer1, layer2, layer3))\n",
    "    \n",
    "    return original_weight, U, S, V\n",
    "\n",
    "def check_layer_condition(layer, condition):\n",
    "    return layer.weight.size(0) > condition\n",
    "\n",
    "def measure_bleu_score(model, tokenizer, data, device):\n",
    "    model.to(device)\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "    for src, ref in data:\n",
    "        input_ids = tokenizer.encode(src, return_tensors='pt').to(device)\n",
    "        output = model.generate(input_ids)\n",
    "        hypothesis = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        references.append([ref.split()])\n",
    "        hypotheses.append(hypothesis.split())\n",
    "    return corpus_bleu(references, hypotheses)\n",
    "\n",
    "def save_decomposition_info(layer_name, rank, loss_of_bleu, compression, initial_params, final_params):\n",
    "    info = {\n",
    "        'layer_name': layer_name,\n",
    "        'rank': rank,\n",
    "        'loss_of_bleu': loss_of_bleu,\n",
    "        'compression': compression,\n",
    "        'initial_params': initial_params,\n",
    "        'final_params': final_params\n",
    "    }\n",
    "    decomposition_info.append(info)\n",
    "\n",
    "def write_decomposition_info_to_file(filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(decomposition_info, f, indent=4)\n",
    "\n",
    "def decompose_transformer(transformer_model, tokenizer, rank, tolerance, condition, eval_examples, eval_data, device):\n",
    "    result = eval_bleu_epoch(args, eval_data, eval_examples, transformer_model, tokenizer, 'test', 'best-bleu') # подсчитывает метрику\n",
    "    initial_bleu = result['bleu']\n",
    "    \n",
    "    for name, module in transformer_model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and check_layer_condition(module, condition):\n",
    "            initial_params = sum(p.numel() for p in module.parameters())\n",
    "            original_weight, U, S, V = decompose_and_replace_linear_layer(transformer_model, name, rank)\n",
    "            final_params = U.numel() + S.numel() + V.numel() # ???+ S.numel()  # S дважды учитывается из-за диагональной матрицы\n",
    "            \n",
    "            # Замеряем компрессию и метрику BLEU\n",
    "            compression = initial_params / final_params\n",
    "            print('compression: ', compression)\n",
    "            result = eval_bleu_epoch(args, eval_data, eval_examples, transformer_model, tokenizer, 'test', 'best-bleu')\n",
    "            bleu_score, test_em = result['bleu'], result['em']\n",
    "            # bleu_score = measure_bleu_score(transformer_model, tokenizer, validation_data, device)\n",
    "            \n",
    "            if bleu_score >= initial_bleu * (1 - tolerance):\n",
    "                print(f\"Decomposed {name} with rank {rank}\")\n",
    "                save_decomposition_info(name, rank, initial_bleu - bleu_score, compression, initial_params, final_params)\n",
    "                model_to_save = transformer_model.module if hasattr(transformer_model, 'module') else transformer_model\n",
    "                output_model_file = os.path.join('sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-decompose/', \"pytorch_model.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                print('Amount of parameters: ', count_parameters(model_to_save))\n",
    "            else:\n",
    "                print(f\"Restoring {name} due to unacceptable BLEU score drop\")\n",
    "                # Восстанавливаем исходные параметры слоя\n",
    "                setattr(transformer_model, name, original_weight)\n",
    "                \n",
    "    # Дообучение модели\n",
    "    # finetune_model(transformer_model)\n",
    "\n",
    "device = torch.device(\"cuda\", 0)\n",
    "pool = multiprocessing.Pool(args.cpu_cont)\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "model.to(args.device)\n",
    "file = 'sh/saved_models/translate/cs-java/codet5p-220m_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin' #os.path.join(args.output_dir, 'checkpoint-{}/pytorch_model.bin'.format(criteria))\n",
    "model.load_state_dict(torch.load(file))\n",
    "eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, pool, tokenizer, 'test',\n",
    "                                                   only_src=True, is_sample=False)\n",
    "rank = 10  # Ранг для декомпозиции\n",
    "tolerance = 0.3  # Порог допустимой потери BLEU\n",
    "condition = 1000  # Условие для выбора слоёв (например, количество параметров больше 1000)\n",
    "\n",
    "decompose_transformer(model, tokenizer, rank, tolerance, condition, eval_examples, eval_data, device)\n",
    "\n",
    "write_decomposition_info_to_file('decomposition_info.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43d40a-8bf5-4a6e-b0ed-c9739557bac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a820682-a111-4a5d-90d7-baae2d349b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
