{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9eb2e77-d003-42ac-ac7d-25b097f1cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import time\n",
    "from dotmap import DotMap\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import T5Config, T5ForConditionalGeneration, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddb0a458-761d-4dab-9102-3cfef81a8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import build_or_load_gen_model\n",
    "from evaluator import smooth_bleu\n",
    "from evaluator.CodeBLEU import calc_code_bleu\n",
    "from evaluator.bleu import _bleu\n",
    "from utils import get_filenames, get_elapse_time, load_and_cache_gen_data\n",
    "from configs import set_seed, set_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33fe10b-c31d-45f4-a646-f30f9333b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def eval_ppl_epoch(args, eval_data, eval_examples, model, tokenizer):\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,\n",
    "                                 num_workers=4, pin_memory=True)\n",
    "    logger.info(\"  \" + \"***** Running ppl evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, batch_num = 0, 0\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Eval ppl\"):\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        source_ids, target_ids = batch\n",
    "        source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "        target_mask = target_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args.model_type == 'roberta':\n",
    "                loss, _, _ = model(source_ids=source_ids, source_mask=source_mask,\n",
    "                                   target_ids=target_ids, target_mask=target_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids=source_ids, attention_mask=source_mask,\n",
    "                                labels=target_ids, decoder_attention_mask=target_mask)\n",
    "                loss = outputs.loss\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "        batch_num += 1\n",
    "    eval_loss = eval_loss / batch_num\n",
    "    eval_ppl = round(np.exp(eval_loss), 5)\n",
    "    return eval_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5190e6-a7ab-45b3-83be-8c7cd5ecc04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, split_tag, criteria):\n",
    "    logger.info(\"  ***** Running bleu evaluation on {} data*****\".format(split_tag))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    if args.data_num == -1:\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size,\n",
    "                                     num_workers=4, pin_memory=True)\n",
    "    else:\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    pred_ids = []\n",
    "    bleu, codebleu = 0.0, 0.0\n",
    "    for batch in tqdm(eval_dataloader, total=len(eval_dataloader), desc=\"Eval bleu for {} set\".format(split_tag)):\n",
    "        source_ids = batch[0].to(args.device)\n",
    "        source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "        with torch.no_grad():\n",
    "            if args.model_type == 'roberta':\n",
    "                preds = model(source_ids=source_ids, source_mask=source_mask)\n",
    "\n",
    "                top_preds = [pred[0].cpu().numpy() for pred in preds]\n",
    "            else:\n",
    "                preds = model.generate(source_ids,\n",
    "                                       attention_mask=source_mask,\n",
    "                                       use_cache=True,\n",
    "                                       num_beams=args.beam_size,\n",
    "                                       early_stopping=args.task == 'summarize',\n",
    "                                       max_length=args.max_target_length)\n",
    "                top_preds = list(preds.cpu().numpy())\n",
    "            pred_ids.extend(top_preds)\n",
    "\n",
    "    pred_nls = [tokenizer.decode(id, skip_special_tokens=True, clean_up_tokenization_spaces=False) for id in pred_ids]\n",
    "\n",
    "    output_fn = os.path.join(args.res_dir, \"test_{}.output\".format(criteria))\n",
    "    gold_fn = os.path.join(args.res_dir, \"test_{}.gold\".format(criteria))\n",
    "    src_fn = os.path.join(args.res_dir, \"test_{}.src\".format(criteria))\n",
    "\n",
    "    if args.task in ['defect']:\n",
    "        target_dict = {0: 'false', 1: 'true'}\n",
    "        golds = [target_dict[ex.target] for ex in eval_examples]\n",
    "        eval_acc = np.mean([int(p == g) for p, g in zip(pred_nls, golds)])\n",
    "        result = {'em': eval_acc * 100, 'bleu': 0, 'codebleu': 0}\n",
    "\n",
    "        with open(output_fn, 'w') as f, open(gold_fn, 'w') as f1, open(src_fn, 'w') as f2:\n",
    "            for pred_nl, gold in zip(pred_nls, eval_examples):\n",
    "                f.write(pred_nl.strip() + '\\n')\n",
    "                f1.write(target_dict[gold.target] + '\\n')\n",
    "                f2.write(gold.source.strip() + '\\n')\n",
    "            logger.info(\"Save the predictions into %s\", output_fn)\n",
    "    else:\n",
    "        dev_accs, predictions = [], []\n",
    "        with open(output_fn, 'w') as f, open(gold_fn, 'w') as f1, open(src_fn, 'w') as f2:\n",
    "            for pred_nl, gold in zip(pred_nls, eval_examples):\n",
    "                dev_accs.append(pred_nl.strip() == gold.target.strip())\n",
    "                if args.task in ['summarize']:\n",
    "                    # for smooth-bleu4 evaluation\n",
    "                    predictions.append(str(gold.idx) + '\\t' + pred_nl)\n",
    "                    f.write(str(gold.idx) + '\\t' + pred_nl.strip() + '\\n')\n",
    "                    f1.write(str(gold.idx) + '\\t' + gold.target.strip() + '\\n')\n",
    "                    f2.write(str(gold.idx) + '\\t' + gold.source.strip() + '\\n')\n",
    "                else:\n",
    "                    f.write(pred_nl.strip() + '\\n')\n",
    "                    f1.write(gold.target.strip() + '\\n')\n",
    "                    f2.write(gold.source.strip() + '\\n')\n",
    "\n",
    "        if args.task == 'summarize':\n",
    "            (goldMap, predictionMap) = smooth_bleu.computeMaps(predictions, gold_fn)\n",
    "            bleu = round(smooth_bleu.bleuFromMaps(goldMap, predictionMap)[0], 2)\n",
    "        else:\n",
    "            bleu = round(_bleu(gold_fn, output_fn), 2)\n",
    "            if args.task in ['concode', 'translate', 'refine']:\n",
    "               codebleu = calc_code_bleu.get_codebleu(gold_fn, output_fn, args.lang)\n",
    "\n",
    "        result = {'em': np.mean(dev_accs) * 100, 'bleu': bleu}\n",
    "        result['codebleu'] = codebleu * 100\n",
    "\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(round(result[key], 4)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f693b1-a5c0-429c-8ccb-0dad00b1bc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_ = 1234\n",
    "random.seed(seed_)\n",
    "np.random.seed(seed_)\n",
    "torch.manual_seed(seed_)\n",
    "torch.cuda.manual_seed_all(seed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c172e33-f549-4878-b15e-c3453f03f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR=\"/home/okozlova/diplom_oksana/CodeT5/CodeT5\"\n",
    "MODEL_TAG = 'codet5_base' # 'codet5_small', 'codet5_large', 'codet5p-220m'\n",
    "MODEL_DIR = 'sh/saved_models'\n",
    "TASK = 'translate'\n",
    "SUB_TASK = 'cs-java'\n",
    "DATA_TAG = 'all'\n",
    "LR = 5e-5\n",
    "BS = 16\n",
    "SRC_LEN = 320\n",
    "TRG_LEN = 256\n",
    "PATIENCE = 5\n",
    "EPOCH = 100\n",
    "FULL_MODEL_TAG=f'{MODEL_TAG}_{DATA_TAG}_lr{LR}_bs{BS}_src{SRC_LEN}_trg{TRG_LEN}_pat{PATIENCE}_e{EPOCH}'\n",
    "OUTPUT_DIR=f'{MODEL_DIR}/{TASK}/{SUB_TASK}/{FULL_MODEL_TAG}'\n",
    "CACHE_DIR=f'{OUTPUT_DIR}/cache_data'\n",
    "RES_DIR=f'{OUTPUT_DIR}/prediction'\n",
    "LOG=f'{OUTPUT_DIR}/train.log'\n",
    "DATA_DIR = '/home/okozlova/diplom_oksana/CodeT5/CodeT5/data/translate/'\n",
    "args_dict = {\n",
    "    \"task\": 'translate',\n",
    "    \"sub_task\": SUB_TASK,\n",
    "    \"lang\": 'java', # 'c_sharp',\n",
    "    \"model_tag\": MODEL_TAG,\n",
    "    \"res_dir\": 'sh/results',\n",
    "    \"model_dir\": 'sh/saved_models',\n",
    "    \"summary_dir\": 'tensorboard',\n",
    "    \"data_num\": -1,\n",
    "    \"gpu\": 0,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_eval_bleu\": True,\n",
    "    \"do_test\": True,\n",
    "    \"model_type\": 'codet5',\n",
    "    \"num_train_epochs\": EPOCH, # 100\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"learning_rate\": LR,\n",
    "    \"patience\": PATIENCE, \n",
    "    \"tokenizer_name\": 'Salesforce/codet5-base',\n",
    "    \"model_name_or_path\": 'Salesforce/codet5-base',\n",
    "    'data_dir': WORKDIR + '/data',\n",
    "    'cache_path': CACHE_DIR,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'summary_dir': 'tensorboard',\n",
    "    'save_last_checkpoints': True,\n",
    "    'always_save_model': True,\n",
    "    'res_dir': RES_DIR,\n",
    "    'train_batch_size': BS,\n",
    "    'eval_batch_size': BS, \n",
    "    'max_source_length': SRC_LEN,\n",
    "    'max_target_length': TRG_LEN,\n",
    "    'seed': 1234,\n",
    "    'local_rank': 0,\n",
    "    'no_cuda': False,\n",
    "    'n_gpu': 1,\n",
    "    'device': torch.device(\"cuda\", 0),\n",
    "    'train_filename': f'{DATA_DIR}/train.java-cs.txt.cs,{DATA_DIR}/train.java-cs.txt.java',\n",
    "    'dev_filename': f'{DATA_DIR}/valid.java-cs.txt.cs,{DATA_DIR}/valid.java-cs.txt.java',\n",
    "    'test_filename': f'{DATA_DIR}/test.java-cs.txt.cs,{DATA_DIR}/test.java-cs.txt.java',\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'gradient_accumulation_steps': 1, # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    'load_model_path': None,\n",
    "    'cpu_cont': 16,\n",
    "    'beam_size': 10\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f5bdb14-f5c1-4f32-8dfb-ff407995cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = DotMap(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565b4a05-71dc-46e5-bbe6-3ed0c10e63d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 12:46:29 - INFO - __main__ -   DotMap(task='translate', sub_task='cs-java', lang='java', model_tag='codet5_base', res_dir='sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/prediction', model_dir='sh/saved_models', summary_dir='tensorboard', data_num=-1, gpu=0, do_train=True, do_eval=True, do_eval_bleu=True, do_test=True, model_type='codet5', num_train_epochs=100, warmup_steps=1000, learning_rate=5e-05, patience=5, tokenizer_name='Salesforce/codet5-base', model_name_or_path='Salesforce/codet5-base', data_dir='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data', cache_path='sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data', output_dir='sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100', save_last_checkpoints=True, always_save_model=True, train_batch_size=16, eval_batch_size=16, max_source_length=320, max_target_length=256, seed=1234, local_rank=0, no_cuda=False, n_gpu=1, device=device(type='cuda', index=0), train_filename='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//train.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//train.java-cs.txt.java', dev_filename='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java', test_filename='/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//test.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//test.java-cs.txt.java', weight_decay=0.0, adam_epsilon=1e-08, gradient_accumulation_steps=1, load_model_path=None, cpu_cont=16, beam_size=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.model_type:  codet5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 12:46:31 - INFO - models -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "05/13/2024 12:46:37 - INFO - utils -   Read 10295 examples, avg src len: 15, avg trg len: 13, max src len: 118, max trg len: 136\n",
      "05/13/2024 12:46:37 - INFO - utils -   [TOKENIZE] avg src len: 56, avg trg len: 45, max src len: 404, max trg len: 391\n",
      "05/13/2024 12:46:37 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/train_all.pt\n",
      "/home/mzhelezin/venvs/otorch/lib/python3.9/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "05/13/2024 12:46:37 - INFO - __main__ -   ***** Running training *****\n",
      "05/13/2024 12:46:37 - INFO - __main__ -     Num examples = 10295\n",
      "05/13/2024 12:46:37 - INFO - __main__ -     Batch size = 16\n",
      "05/13/2024 12:46:37 - INFO - __main__ -     Batch num = 644\n",
      "05/13/2024 12:46:37 - INFO - __main__ -     Num epoch = 100\n",
      "[0] Train loss 0.17: 100%|████████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:25<00:00,  1.98it/s]\n",
      "05/13/2024 12:52:02 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 12:52:02 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/dev_all.pt\n",
      "05/13/2024 12:52:02 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 12:52:02 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 12:52:02 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.05it/s]\n",
      "05/13/2024 12:52:08 - INFO - __main__ -     epoch = 0\n",
      "05/13/2024 12:52:08 - INFO - __main__ -     eval_ppl = 1.06111\n",
      "05/13/2024 12:52:08 - INFO - __main__ -     global_step = 644\n",
      "05/13/2024 12:52:08 - INFO - __main__ -     ********************\n",
      "05/13/2024 12:52:09 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 12:52:09 - INFO - __main__ -     Best ppl:1.06111\n",
      "05/13/2024 12:52:09 - INFO - __main__ -     ********************\n",
      "05/13/2024 12:52:09 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 12:52:09 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 12:52:10 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 12:52:10 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 363.21it/s]\n",
      "05/13/2024 12:52:11 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 12:52:11 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 12:52:11 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:53<00:00,  5.43s/it]\n",
      "05/13/2024 12:55:10 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 12:55:10 - INFO - __main__ -     bleu = 66.38\n",
      "05/13/2024 12:55:10 - INFO - __main__ -     codebleu = 74.029\n",
      "05/13/2024 12:55:10 - INFO - __main__ -     em = 52.505\n",
      "05/13/2024 12:55:10 - INFO - __main__ -     [0] Best bleu+em: 118.89 (bleu: 66.38, em: 52.51)\n",
      "05/13/2024 12:55:10 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.6637586094527332, weighted ngram match: 0.6683766329126941, syntax_match: 0.8302033751622674, dataflow_match: 0.798823016564952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 12:55:11 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 12:55:11 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[1] Train loss 0.055: 100%|███████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.96it/s]\n",
      "05/13/2024 13:00:39 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:00:39 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:00:39 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 13:00:45 - INFO - __main__ -     epoch = 1\n",
      "05/13/2024 13:00:45 - INFO - __main__ -     eval_ppl = 1.04429\n",
      "05/13/2024 13:00:45 - INFO - __main__ -     global_step = 1288\n",
      "05/13/2024 13:00:45 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:00:46 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:00:46 - INFO - __main__ -     Best ppl:1.04429\n",
      "05/13/2024 13:00:46 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:00:46 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:00:46 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:00:47 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:00:47 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 354.50it/s]\n",
      "05/13/2024 13:00:48 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:00:48 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:00:48 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:52<00:00,  5.39s/it]\n",
      "05/13/2024 13:03:46 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:03:46 - INFO - __main__ -     bleu = 73.05\n",
      "05/13/2024 13:03:46 - INFO - __main__ -     codebleu = 79.0456\n",
      "05/13/2024 13:03:46 - INFO - __main__ -     em = 60.1202\n",
      "05/13/2024 13:03:46 - INFO - __main__ -     [1] Best bleu+em: 133.17 (bleu: 73.05, em: 60.12)\n",
      "05/13/2024 13:03:46 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7304604693411234, weighted ngram match: 0.7357119208223809, syntax_match: 0.8632626568585028, dataflow_match: 0.8323888404533566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 13:03:47 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 13:03:47 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[2] Train loss 0.04: 100%|████████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.96it/s]\n",
      "05/13/2024 13:09:15 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:09:15 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:09:15 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 13:09:20 - INFO - __main__ -     epoch = 2\n",
      "05/13/2024 13:09:20 - INFO - __main__ -     eval_ppl = 1.03917\n",
      "05/13/2024 13:09:20 - INFO - __main__ -     global_step = 1932\n",
      "05/13/2024 13:09:20 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:09:21 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:09:21 - INFO - __main__ -     Best ppl:1.03917\n",
      "05/13/2024 13:09:21 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:09:22 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:09:22 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:09:22 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:09:22 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 337.93it/s]\n",
      "05/13/2024 13:09:24 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:09:24 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:09:24 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:48<00:00,  5.26s/it]\n",
      "05/13/2024 13:12:18 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:12:18 - INFO - __main__ -     bleu = 75.13\n",
      "05/13/2024 13:12:18 - INFO - __main__ -     codebleu = 81.492\n",
      "05/13/2024 13:12:18 - INFO - __main__ -     em = 60.7214\n",
      "05/13/2024 13:12:18 - INFO - __main__ -     [2] Best bleu+em: 135.85 (bleu: 75.13, em: 60.72)\n",
      "05/13/2024 13:12:18 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7512912518720135, weighted ngram match: 0.756249297194844, syntax_match: 0.8820424058848984, dataflow_match: 0.8700959023539668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 13:12:19 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 13:12:19 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[3] Train loss 0.03: 100%|████████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.96it/s]\n",
      "05/13/2024 13:17:47 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:17:47 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:17:47 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 13:17:52 - INFO - __main__ -     epoch = 3\n",
      "05/13/2024 13:17:52 - INFO - __main__ -     eval_ppl = 1.03704\n",
      "05/13/2024 13:17:52 - INFO - __main__ -     global_step = 2576\n",
      "05/13/2024 13:17:52 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:17:53 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:17:53 - INFO - __main__ -     Best ppl:1.03704\n",
      "05/13/2024 13:17:53 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:17:54 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:17:54 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:17:54 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:17:54 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 361.01it/s]\n",
      "05/13/2024 13:17:56 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:17:56 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:17:56 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:49<00:00,  5.31s/it]\n",
      "05/13/2024 13:20:51 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:20:51 - INFO - __main__ -     bleu = 76.6\n",
      "05/13/2024 13:20:51 - INFO - __main__ -     codebleu = 82.48\n",
      "05/13/2024 13:20:51 - INFO - __main__ -     em = 64.5291\n",
      "05/13/2024 13:20:51 - INFO - __main__ -     [3] Best bleu+em: 141.13 (bleu: 76.60, em: 64.53)\n",
      "05/13/2024 13:20:51 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7659669577119461, weighted ngram match: 0.7708417258217134, syntax_match: 0.8925140631761143, dataflow_match: 0.8698779424585876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 13:20:52 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 13:20:52 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[4] Train loss 0.023: 100%|███████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 13:26:20 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:26:20 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:26:20 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 13:26:25 - INFO - __main__ -     epoch = 4\n",
      "05/13/2024 13:26:25 - INFO - __main__ -     eval_ppl = 1.03543\n",
      "05/13/2024 13:26:25 - INFO - __main__ -     global_step = 3220\n",
      "05/13/2024 13:26:25 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:26:26 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:26:26 - INFO - __main__ -     Best ppl:1.03543\n",
      "05/13/2024 13:26:26 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:26:27 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:26:27 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:26:27 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:26:27 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 366.92it/s]\n",
      "05/13/2024 13:26:29 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:26:29 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:26:29 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:44<00:00,  5.15s/it]\n",
      "05/13/2024 13:29:19 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:29:19 - INFO - __main__ -     bleu = 78.29\n",
      "05/13/2024 13:29:19 - INFO - __main__ -     codebleu = 84.0653\n",
      "05/13/2024 13:29:19 - INFO - __main__ -     em = 63.9279\n",
      "05/13/2024 13:29:19 - INFO - __main__ -     [4] Best bleu+em: 142.22 (bleu: 78.29, em: 63.93)\n",
      "05/13/2024 13:29:19 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7829057855080891, weighted ngram match: 0.7880062920103581, syntax_match: 0.8967546516659455, dataflow_match: 0.8949433304272014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 13:29:20 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 13:29:20 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[5] Train loss 0.019: 100%|███████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 13:34:48 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:34:48 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:34:48 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 13:34:53 - INFO - __main__ -     epoch = 5\n",
      "05/13/2024 13:34:53 - INFO - __main__ -     eval_ppl = 1.03538\n",
      "05/13/2024 13:34:53 - INFO - __main__ -     global_step = 3864\n",
      "05/13/2024 13:34:53 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:34:54 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:34:54 - INFO - __main__ -     Best ppl:1.03538\n",
      "05/13/2024 13:34:54 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:34:55 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:34:55 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:34:55 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:34:55 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 366.71it/s]\n",
      "05/13/2024 13:34:57 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:34:57 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:34:57 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:47<00:00,  5.24s/it]\n",
      "05/13/2024 13:37:50 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:37:50 - INFO - __main__ -     bleu = 78.59\n",
      "05/13/2024 13:37:50 - INFO - __main__ -     codebleu = 84.4701\n",
      "05/13/2024 13:37:50 - INFO - __main__ -     em = 65.3307\n",
      "05/13/2024 13:37:50 - INFO - __main__ -     [5] Best bleu+em: 143.92 (bleu: 78.59, em: 65.33)\n",
      "05/13/2024 13:37:50 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7858288561206691, weighted ngram match: 0.790895052193847, syntax_match: 0.9016875811337084, dataflow_match: 0.9003923278116827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 13:37:51 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 13:37:51 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[6] Train loss 0.015: 100%|███████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 13:43:18 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:43:18 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:43:18 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 13:43:24 - INFO - __main__ -     epoch = 6\n",
      "05/13/2024 13:43:24 - INFO - __main__ -     eval_ppl = 1.03521\n",
      "05/13/2024 13:43:24 - INFO - __main__ -     global_step = 4508\n",
      "05/13/2024 13:43:24 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:43:25 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:43:25 - INFO - __main__ -     Best ppl:1.03521\n",
      "05/13/2024 13:43:25 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:43:25 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:43:25 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:43:26 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:43:26 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 367.13it/s]\n",
      "05/13/2024 13:43:27 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:43:27 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:43:27 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:51<00:00,  5.37s/it]\n",
      "05/13/2024 13:46:24 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:46:24 - INFO - __main__ -     bleu = 77.99\n",
      "05/13/2024 13:46:24 - INFO - __main__ -     codebleu = 83.8814\n",
      "05/13/2024 13:46:24 - INFO - __main__ -     em = 65.1303\n",
      "05/13/2024 13:46:24 - INFO - __main__ -   Bleu does not increase for 1 epochs\n",
      "05/13/2024 13:46:24 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7798880788481343, weighted ngram match: 0.7845152807024449, syntax_match: 0.8991778450887062, dataflow_match: 0.8916739319965127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[7] Train loss 0.012: 100%|███████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 13:51:52 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 13:51:52 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:51:52 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 13:51:58 - INFO - __main__ -     epoch = 7\n",
      "05/13/2024 13:51:58 - INFO - __main__ -     eval_ppl = 1.03473\n",
      "05/13/2024 13:51:58 - INFO - __main__ -     global_step = 5152\n",
      "05/13/2024 13:51:58 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:51:59 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 13:51:59 - INFO - __main__ -     Best ppl:1.03473\n",
      "05/13/2024 13:51:59 - INFO - __main__ -     ********************\n",
      "05/13/2024 13:52:00 - INFO - __main__ -   Save the best ppl model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-ppl/pytorch_model.bin\n",
      "05/13/2024 13:52:00 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 13:52:00 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 13:52:00 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 367.84it/s]\n",
      "05/13/2024 13:52:01 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 13:52:01 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 13:52:01 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:55<00:00,  5.48s/it]\n",
      "05/13/2024 13:55:02 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 13:55:02 - INFO - __main__ -     bleu = 78.48\n",
      "05/13/2024 13:55:02 - INFO - __main__ -     codebleu = 83.4371\n",
      "05/13/2024 13:55:02 - INFO - __main__ -     em = 65.9319\n",
      "05/13/2024 13:55:02 - INFO - __main__ -     [7] Best bleu+em: 144.41 (bleu: 78.48, em: 65.93)\n",
      "05/13/2024 13:55:02 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7848160427066382, weighted ngram match: 0.7898810565084641, syntax_match: 0.8922544353093899, dataflow_match: 0.8705318221447254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 13:55:03 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 13:55:03 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[8] Train loss 0.01: 100%|████████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:00:31 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:00:31 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:00:31 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 14:00:36 - INFO - __main__ -     epoch = 8\n",
      "05/13/2024 14:00:36 - INFO - __main__ -     eval_ppl = 1.03612\n",
      "05/13/2024 14:00:36 - INFO - __main__ -     global_step = 5796\n",
      "05/13/2024 14:00:36 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:00:37 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:00:37 - INFO - __main__ -   Ppl does not decrease for 1 epochs\n",
      "05/13/2024 14:00:37 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:00:37 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:00:37 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 339.78it/s]\n",
      "05/13/2024 14:00:39 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:00:39 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:00:39 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:56<00:00,  5.53s/it]\n",
      "05/13/2024 14:03:41 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:03:41 - INFO - __main__ -     bleu = 79.05\n",
      "05/13/2024 14:03:41 - INFO - __main__ -     codebleu = 84.4674\n",
      "05/13/2024 14:03:41 - INFO - __main__ -     em = 65.3307\n",
      "05/13/2024 14:03:41 - INFO - __main__ -   Bleu does not increase for 1 epochs\n",
      "05/13/2024 14:03:41 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.790452285534264, weighted ngram match: 0.7953546494024162, syntax_match: 0.8946776287321506, dataflow_match: 0.8982127288578902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[9] Train loss 0.009: 100%|███████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:09:09 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:09:09 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:09:09 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 14:09:14 - INFO - __main__ -     epoch = 9\n",
      "05/13/2024 14:09:14 - INFO - __main__ -     eval_ppl = 1.0366\n",
      "05/13/2024 14:09:14 - INFO - __main__ -     global_step = 6440\n",
      "05/13/2024 14:09:14 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:09:15 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:09:15 - INFO - __main__ -   Ppl does not decrease for 2 epochs\n",
      "05/13/2024 14:09:15 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:09:15 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:09:15 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 367.96it/s]\n",
      "05/13/2024 14:09:17 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:09:17 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:09:17 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:01<00:00,  5.67s/it]\n",
      "05/13/2024 14:12:24 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:12:24 - INFO - __main__ -     bleu = 78.16\n",
      "05/13/2024 14:12:24 - INFO - __main__ -     codebleu = 83.9795\n",
      "05/13/2024 14:12:24 - INFO - __main__ -     em = 64.5291\n",
      "05/13/2024 14:12:24 - INFO - __main__ -   Bleu does not increase for 2 epochs\n",
      "05/13/2024 14:12:24 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7816054146299463, weighted ngram match: 0.7861170532459663, syntax_match: 0.8997836434443963, dataflow_match: 0.8916739319965127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10] Train loss 0.007: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:17:52 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:17:52 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:17:52 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 14:17:57 - INFO - __main__ -     epoch = 10\n",
      "05/13/2024 14:17:57 - INFO - __main__ -     eval_ppl = 1.03884\n",
      "05/13/2024 14:17:57 - INFO - __main__ -     global_step = 7084\n",
      "05/13/2024 14:17:57 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:17:58 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:17:58 - INFO - __main__ -   Ppl does not decrease for 3 epochs\n",
      "05/13/2024 14:17:58 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:17:58 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:17:58 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 368.32it/s]\n",
      "05/13/2024 14:17:59 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:17:59 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:17:59 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:58<00:00,  5.58s/it]\n",
      "05/13/2024 14:21:04 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:21:04 - INFO - __main__ -     bleu = 79.2\n",
      "05/13/2024 14:21:04 - INFO - __main__ -     codebleu = 84.7904\n",
      "05/13/2024 14:21:04 - INFO - __main__ -     em = 65.3307\n",
      "05/13/2024 14:21:04 - INFO - __main__ -     [10] Best bleu+em: 144.53 (bleu: 79.20, em: 65.33)\n",
      "05/13/2024 14:21:04 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.791960713520326, weighted ngram match: 0.7978910853341352, syntax_match: 0.903331890956296, dataflow_match: 0.8984306887532694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 14:21:05 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 14:21:05 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[11] Train loss 0.006: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:26:32 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:26:32 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:26:32 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 14:26:38 - INFO - __main__ -     epoch = 11\n",
      "05/13/2024 14:26:38 - INFO - __main__ -     eval_ppl = 1.03865\n",
      "05/13/2024 14:26:38 - INFO - __main__ -     global_step = 7728\n",
      "05/13/2024 14:26:38 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:26:38 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:26:38 - INFO - __main__ -   Ppl does not decrease for 4 epochs\n",
      "05/13/2024 14:26:38 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:26:39 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:26:39 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 367.99it/s]\n",
      "05/13/2024 14:26:40 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:26:40 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:26:40 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:53<00:00,  5.42s/it]\n",
      "05/13/2024 14:29:39 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:29:39 - INFO - __main__ -     bleu = 79.02\n",
      "05/13/2024 14:29:39 - INFO - __main__ -     codebleu = 84.8152\n",
      "05/13/2024 14:29:39 - INFO - __main__ -     em = 65.3307\n",
      "05/13/2024 14:29:39 - INFO - __main__ -   Bleu does not increase for 1 epochs\n",
      "05/13/2024 14:29:39 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7901473952241362, weighted ngram match: 0.7984051181416916, syntax_match: 0.8999567286888793, dataflow_match: 0.9040976460331299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12] Train loss 0.005: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:35:07 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:35:07 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:35:07 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 14:35:12 - INFO - __main__ -     epoch = 12\n",
      "05/13/2024 14:35:12 - INFO - __main__ -     eval_ppl = 1.03892\n",
      "05/13/2024 14:35:12 - INFO - __main__ -     global_step = 8372\n",
      "05/13/2024 14:35:12 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:35:13 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:35:13 - INFO - __main__ -   Ppl does not decrease for 5 epochs\n",
      "05/13/2024 14:35:13 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:35:14 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:35:14 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 368.40it/s]\n",
      "05/13/2024 14:35:15 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:35:15 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:35:15 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [03:04<00:00,  5.76s/it]\n",
      "05/13/2024 14:38:25 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:38:25 - INFO - __main__ -     bleu = 79.25\n",
      "05/13/2024 14:38:25 - INFO - __main__ -     codebleu = 84.4049\n",
      "05/13/2024 14:38:25 - INFO - __main__ -     em = 65.9319\n",
      "05/13/2024 14:38:25 - INFO - __main__ -     [12] Best bleu+em: 145.18 (bleu: 79.25, em: 65.93)\n",
      "05/13/2024 14:38:25 - INFO - __main__ -     ********************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7924721221980376, weighted ngram match: 0.7981688962035566, syntax_match: 0.8960623106880139, dataflow_match: 0.8894943330427202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 14:38:26 - INFO - __main__ -   Save the best bleu model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 14:38:26 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "[13] Train loss 0.005: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:43:54 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:43:54 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:43:54 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 14:43:59 - INFO - __main__ -     epoch = 13\n",
      "05/13/2024 14:43:59 - INFO - __main__ -     eval_ppl = 1.03949\n",
      "05/13/2024 14:43:59 - INFO - __main__ -     global_step = 9016\n",
      "05/13/2024 14:43:59 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:44:00 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:44:00 - INFO - __main__ -   Ppl does not decrease for 6 epochs\n",
      "05/13/2024 14:44:00 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:44:00 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:44:00 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 352.75it/s]\n",
      "05/13/2024 14:44:02 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:44:02 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:44:02 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:54<00:00,  5.44s/it]\n",
      "05/13/2024 14:47:01 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:47:01 - INFO - __main__ -     bleu = 79.36\n",
      "05/13/2024 14:47:01 - INFO - __main__ -     codebleu = 84.4743\n",
      "05/13/2024 14:47:01 - INFO - __main__ -     em = 64.3287\n",
      "05/13/2024 14:47:01 - INFO - __main__ -   Bleu does not increase for 1 epochs\n",
      "05/13/2024 14:47:01 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7935825838606049, weighted ngram match: 0.7995637868413059, syntax_match: 0.8950237992211164, dataflow_match: 0.8908020924149956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14] Train loss 0.004: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 14:52:29 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 14:52:29 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:52:29 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 14:52:35 - INFO - __main__ -     epoch = 14\n",
      "05/13/2024 14:52:35 - INFO - __main__ -     eval_ppl = 1.04101\n",
      "05/13/2024 14:52:35 - INFO - __main__ -     global_step = 9660\n",
      "05/13/2024 14:52:35 - INFO - __main__ -     ********************\n",
      "05/13/2024 14:52:35 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 14:52:35 - INFO - __main__ -   Ppl does not decrease for 7 epochs\n",
      "05/13/2024 14:52:35 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 14:52:36 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 14:52:36 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 368.03it/s]\n",
      "05/13/2024 14:52:37 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 14:52:37 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 14:52:37 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:59<00:00,  5.60s/it]\n",
      "05/13/2024 14:55:42 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 14:55:42 - INFO - __main__ -     bleu = 79.79\n",
      "05/13/2024 14:55:42 - INFO - __main__ -     codebleu = 85.0205\n",
      "05/13/2024 14:55:42 - INFO - __main__ -     em = 64.9299\n",
      "05/13/2024 14:55:42 - INFO - __main__ -   Bleu does not increase for 2 epochs\n",
      "05/13/2024 14:55:42 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7978330652346486, weighted ngram match: 0.8056532019343771, syntax_match: 0.9010817827780182, dataflow_match: 0.8962510897994769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15] Train loss 0.004: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 15:01:10 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 15:01:10 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:01:10 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 15:01:15 - INFO - __main__ -     epoch = 15\n",
      "05/13/2024 15:01:15 - INFO - __main__ -     eval_ppl = 1.04158\n",
      "05/13/2024 15:01:15 - INFO - __main__ -     global_step = 10304\n",
      "05/13/2024 15:01:15 - INFO - __main__ -     ********************\n",
      "05/13/2024 15:01:16 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 15:01:16 - INFO - __main__ -   Ppl does not decrease for 8 epochs\n",
      "05/13/2024 15:01:16 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 15:01:16 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 15:01:16 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 359.73it/s]\n",
      "05/13/2024 15:01:18 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 15:01:18 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:01:18 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:48<00:00,  5.26s/it]\n",
      "05/13/2024 15:04:11 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 15:04:11 - INFO - __main__ -     bleu = 78.5\n",
      "05/13/2024 15:04:11 - INFO - __main__ -     codebleu = 83.8976\n",
      "05/13/2024 15:04:11 - INFO - __main__ -     em = 64.9299\n",
      "05/13/2024 15:04:11 - INFO - __main__ -   Bleu does not increase for 3 epochs\n",
      "05/13/2024 15:04:11 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7849812824163174, weighted ngram match: 0.7894414247304992, syntax_match: 0.898745131977499, dataflow_match: 0.8827375762859634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16] Train loss 0.004: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 15:09:39 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 15:09:39 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:09:39 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.03it/s]\n",
      "05/13/2024 15:09:44 - INFO - __main__ -     epoch = 16\n",
      "05/13/2024 15:09:44 - INFO - __main__ -     eval_ppl = 1.04026\n",
      "05/13/2024 15:09:44 - INFO - __main__ -     global_step = 10948\n",
      "05/13/2024 15:09:44 - INFO - __main__ -     ********************\n",
      "05/13/2024 15:09:45 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 15:09:45 - INFO - __main__ -   Ppl does not decrease for 9 epochs\n",
      "05/13/2024 15:09:45 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 15:09:46 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 15:09:46 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 368.61it/s]\n",
      "05/13/2024 15:09:47 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 15:09:47 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:09:47 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:57<00:00,  5.55s/it]\n",
      "05/13/2024 15:12:50 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 15:12:50 - INFO - __main__ -     bleu = 78.78\n",
      "05/13/2024 15:12:50 - INFO - __main__ -     codebleu = 84.0028\n",
      "05/13/2024 15:12:50 - INFO - __main__ -     em = 64.1283\n",
      "05/13/2024 15:12:50 - INFO - __main__ -   Bleu does not increase for 4 epochs\n",
      "05/13/2024 15:12:50 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7878155188817185, weighted ngram match: 0.7921267247250896, syntax_match: 0.8937256598874946, dataflow_match: 0.8864428945074107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 15:18:18 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 15:18:18 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:18:18 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 15:18:23 - INFO - __main__ -     epoch = 17\n",
      "05/13/2024 15:18:23 - INFO - __main__ -     eval_ppl = 1.04261\n",
      "05/13/2024 15:18:23 - INFO - __main__ -     global_step = 11592\n",
      "05/13/2024 15:18:23 - INFO - __main__ -     ********************\n",
      "05/13/2024 15:18:24 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 15:18:24 - INFO - __main__ -   Ppl does not decrease for 10 epochs\n",
      "05/13/2024 15:18:24 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 15:18:24 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 15:18:24 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 368.39it/s]\n",
      "05/13/2024 15:18:25 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 15:18:25 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:18:25 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:48<00:00,  5.25s/it]\n",
      "05/13/2024 15:21:19 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 15:21:19 - INFO - __main__ -     bleu = 78.82\n",
      "05/13/2024 15:21:19 - INFO - __main__ -     codebleu = 84.548\n",
      "05/13/2024 15:21:19 - INFO - __main__ -     em = 65.9319\n",
      "05/13/2024 15:21:19 - INFO - __main__ -   Bleu does not increase for 5 epochs\n",
      "05/13/2024 15:21:19 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7881985513676216, weighted ngram match: 0.7927758468749043, syntax_match: 0.8959757680657724, dataflow_match: 0.9049694856146469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[18] Train loss 0.003: 100%|██████████████████████████████████████████████████████████████████████████████████████| 644/644 [05:27<00:00,  1.97it/s]\n",
      "05/13/2024 15:26:46 - INFO - __main__ -     ***** Running ppl evaluation *****\n",
      "05/13/2024 15:26:46 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:26:46 - INFO - __main__ -     Batch size = 16\n",
      "Eval ppl: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05<00:00,  6.04it/s]\n",
      "05/13/2024 15:26:52 - INFO - __main__ -     epoch = 18\n",
      "05/13/2024 15:26:52 - INFO - __main__ -     eval_ppl = 1.04231\n",
      "05/13/2024 15:26:52 - INFO - __main__ -     global_step = 12236\n",
      "05/13/2024 15:26:52 - INFO - __main__ -     ********************\n",
      "05/13/2024 15:26:53 - INFO - __main__ -   Save the last model into sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-last/pytorch_model.bin\n",
      "05/13/2024 15:26:53 - INFO - __main__ -   Ppl does not decrease for 11 epochs\n",
      "05/13/2024 15:26:53 - INFO - __main__ -   ***** CUDA.empty_cache() *****\n",
      "05/13/2024 15:26:53 - INFO - utils -   Read 499 examples, avg src len: 16, avg trg len: 15, max src len: 99, max trg len: 109\n",
      "05/13/2024 15:26:53 - INFO - utils -   Sample 5k data for computing bleu from /home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.cs,/home/mzhelezin/diplom_oksana/CodeT5/CodeT5/data/translate//valid.java-cs.txt.java\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [00:01<00:00, 363.65it/s]\n",
      "05/13/2024 15:26:54 - INFO - __main__ -     ***** Running bleu evaluation on dev data*****\n",
      "05/13/2024 15:26:54 - INFO - __main__ -     Num examples = 499\n",
      "05/13/2024 15:26:54 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for dev set: 100%|████████████████████████████████████████████████████████████████████████████████████████| 32/32 [02:50<00:00,  5.33s/it]\n",
      "05/13/2024 15:29:50 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 15:29:50 - INFO - __main__ -     bleu = 78.41\n",
      "05/13/2024 15:29:50 - INFO - __main__ -     codebleu = 84.3647\n",
      "05/13/2024 15:29:50 - INFO - __main__ -     em = 65.3307\n",
      "05/13/2024 15:29:50 - INFO - __main__ -   Bleu does not increase for 6 epochs\n",
      "05/13/2024 15:29:50 - INFO - __main__ -   [18] Early stop as not_bleu_em_inc_cnt=6, and not_loss_dec_cnt=11\n",
      "\n",
      "05/13/2024 15:29:50 - INFO - __main__ -   Finish training and take 2h43m\n",
      "05/13/2024 15:29:50 - INFO - __main__ -     ***** Testing *****\n",
      "05/13/2024 15:29:50 - INFO - __main__ -     Batch size = 16\n",
      "05/13/2024 15:29:50 - INFO - __main__ -   Reload model from sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7840599602002781, weighted ngram match: 0.7894498714684238, syntax_match: 0.9013414106447425, dataflow_match: 0.8997384481255449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 15:29:51 - INFO - utils -   Read 1000 examples, avg src len: 14, avg trg len: 13, max src len: 94, max trg len: 98\n",
      "05/13/2024 15:29:51 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/test_src_all.pt\n",
      "05/13/2024 15:29:51 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "05/13/2024 15:29:51 - INFO - __main__ -     Num examples = 1000\n",
      "05/13/2024 15:29:51 - INFO - __main__ -     Batch size = 16\n",
      "Eval bleu for test set: 100%|███████████████████████████████████████████████████████████████████████████████████████| 63/63 [04:51<00:00,  4.62s/it]\n",
      "05/13/2024 15:34:52 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 15:34:52 - INFO - __main__ -     bleu = 79.08\n",
      "05/13/2024 15:34:52 - INFO - __main__ -     codebleu = 84.8959\n",
      "05/13/2024 15:34:52 - INFO - __main__ -     em = 66.6\n",
      "05/13/2024 15:34:52 - INFO - __main__ -   [best-bleu] bleu-4: 79.08, em: 66.6000, codebleu: 84.8959\n",
      "\n",
      "05/13/2024 15:34:52 - INFO - __main__ -   Finish and take 2h48m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7907789938454809, weighted ngram match: 0.799421932584435, syntax_match: 0.9048335527553132, dataflow_match: 0.9007996001999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-13:\n",
      "Process ForkPoolWorker-12:\n"
     ]
    }
   ],
   "source": [
    "logger.info(args)\n",
    "t0 = time.time()\n",
    "\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "model.to(args.device)\n",
    "if args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "pool = multiprocessing.Pool(args.cpu_cont)\n",
    "fa = open(os.path.join(args.output_dir, 'summary.log'), 'a+')\n",
    "\n",
    "if args.do_train:\n",
    "    if args.local_rank in [-1, 0] and args.data_num == -1:\n",
    "        summary_fn = '{}/{}'.format(args.summary_dir, '/'.join(args.output_dir.split('/')[1:]))\n",
    "        tb_writer = SummaryWriter(summary_fn)\n",
    "\n",
    "    train_examples, train_data = load_and_cache_gen_data(args, args.train_filename, pool, tokenizer, 'train')\n",
    "    train_sampler = RandomSampler(train_data) \n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size,\n",
    "                                  num_workers=4, pin_memory=True)\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    num_train_optimization_steps = args.num_train_epochs * len(train_dataloader)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=args.warmup_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "    # Start training\n",
    "    train_example_num = len(train_data)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", train_example_num)\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Batch num = %d\", math.ceil(train_example_num / args.train_batch_size))\n",
    "    logger.info(\"  Num epoch = %d\", args.num_train_epochs)\n",
    "\n",
    "    dev_dataset = {}\n",
    "    global_step, best_bleu_em, best_ppl = 0, -1, 1e6\n",
    "    not_loss_dec_cnt, not_bleu_em_inc_cnt = 0, 0 if args.do_eval_bleu else 1e6\n",
    "\n",
    "    for cur_epoch in range(0, int(args.num_train_epochs)):\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
    "        nb_tr_examples, nb_tr_steps, tr_loss = 0, 0, 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(bar):\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            source_ids, target_ids = batch\n",
    "            source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "            target_mask = target_ids.ne(tokenizer.pad_token_id)\n",
    "\n",
    "            if args.model_type == 'roberta':\n",
    "                loss, _, _ = model(source_ids=source_ids, source_mask=source_mask,\n",
    "                                   target_ids=target_ids, target_mask=target_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids=source_ids, attention_mask=source_mask,\n",
    "                                labels=target_ids, decoder_attention_mask=target_mask)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            nb_tr_examples += source_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            loss.backward()\n",
    "\n",
    "            if nb_tr_steps % args.gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "                train_loss = round(tr_loss * args.gradient_accumulation_steps / (nb_tr_steps + 1), 4)\n",
    "                bar.set_description(\"[{}] Train loss {}\".format(cur_epoch, round(train_loss, 3)))\n",
    "\n",
    "        if args.do_eval:\n",
    "            if 'dev_loss' in dev_dataset:\n",
    "                eval_examples, eval_data = dev_dataset['dev_loss']\n",
    "            else:\n",
    "                eval_examples, eval_data = load_and_cache_gen_data(args, args.dev_filename, pool, tokenizer, 'dev')\n",
    "                dev_dataset['dev_loss'] = eval_examples, eval_data\n",
    "\n",
    "            eval_ppl = eval_ppl_epoch(args, eval_data, eval_examples, model, tokenizer)\n",
    "            result = {'epoch': cur_epoch, 'global_step': global_step, 'eval_ppl': eval_ppl}\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            logger.info(\"  \" + \"*\" * 20)\n",
    "            if args.data_num == -1:\n",
    "                tb_writer.add_scalar('dev_ppl', eval_ppl, cur_epoch)\n",
    "\n",
    "            if args.save_last_checkpoints:\n",
    "                last_output_dir = os.path.join(args.output_dir, 'checkpoint-last')\n",
    "                if not os.path.exists(last_output_dir):\n",
    "                    os.makedirs(last_output_dir)\n",
    "                model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                output_model_file = os.path.join(last_output_dir, \"pytorch_model.bin\")\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                logger.info(\"Save the last model into %s\", output_model_file)\n",
    "\n",
    "            if eval_ppl < best_ppl:\n",
    "                not_loss_dec_cnt = 0\n",
    "                logger.info(\"  Best ppl:%s\", eval_ppl)\n",
    "                logger.info(\"  \" + \"*\" * 20)\n",
    "                fa.write(\"[%d] Best ppl changed into %.4f\\n\" % (cur_epoch, eval_ppl))\n",
    "                best_ppl = eval_ppl\n",
    "\n",
    "                output_dir = os.path.join(args.output_dir, 'checkpoint-best-ppl')\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                if args.always_save_model:\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                    output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "                    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                    logger.info(\"Save the best ppl model into %s\", output_model_file)\n",
    "            else:\n",
    "                not_loss_dec_cnt += 1\n",
    "                logger.info(\"Ppl does not decrease for %d epochs\", not_loss_dec_cnt)\n",
    "                if all([x > args.patience for x in [not_bleu_em_inc_cnt, not_loss_dec_cnt]]):\n",
    "                    early_stop_str = \"[%d] Early stop as not_bleu_em_inc_cnt=%d, and not_loss_dec_cnt=%d\\n\" % (\n",
    "                        cur_epoch, not_bleu_em_inc_cnt, not_loss_dec_cnt)\n",
    "                    logger.info(early_stop_str)\n",
    "                    fa.write(early_stop_str)\n",
    "                    break\n",
    "            logger.info(\"***** CUDA.empty_cache() *****\")\n",
    "            torch.cuda.empty_cache()\n",
    "            if args.do_eval_bleu:\n",
    "                eval_examples, eval_data = load_and_cache_gen_data(args, args.dev_filename, pool, tokenizer, 'dev',\n",
    "                                                                   only_src=True, is_sample=True)\n",
    "\n",
    "                result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'dev', 'e%d' % cur_epoch)\n",
    "                dev_bleu, dev_em = result['bleu'], result['em']\n",
    "                if args.task in ['summarize']:\n",
    "                    dev_bleu_em = dev_bleu\n",
    "                elif args.task in ['defect']:\n",
    "                    dev_bleu_em = dev_em\n",
    "                else:\n",
    "                    dev_bleu_em = dev_bleu + dev_em\n",
    "                if args.data_num == -1:\n",
    "                    tb_writer.add_scalar('dev_bleu_em', dev_bleu_em, cur_epoch)\n",
    "                    # tb_writer.add_scalar('dev_em', dev_em, cur_epoch)\n",
    "                if dev_bleu_em > best_bleu_em:\n",
    "                    not_bleu_em_inc_cnt = 0\n",
    "                    logger.info(\"  [%d] Best bleu+em: %.2f (bleu: %.2f, em: %.2f)\",\n",
    "                                cur_epoch, dev_bleu_em, dev_bleu, dev_em)\n",
    "                    logger.info(\"  \" + \"*\" * 20)\n",
    "                    best_bleu_em = dev_bleu_em\n",
    "                    fa.write(\"[%d] Best bleu+em changed into %.2f (bleu: %.2f, em: %.2f)\\n\" % (\n",
    "                        cur_epoch, best_bleu_em, dev_bleu, dev_em))\n",
    "                    # Save best checkpoint for best bleu\n",
    "                    output_dir = os.path.join(args.output_dir, 'checkpoint-best-bleu')\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    if args.data_num == -1 or args.always_save_model:\n",
    "                        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "                        output_model_file = os.path.join(output_dir, \"pytorch_model.bin\")\n",
    "                        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "                        logger.info(\"Save the best bleu model into %s\", output_model_file)\n",
    "                else:\n",
    "                    not_bleu_em_inc_cnt += 1\n",
    "                    logger.info(\"Bleu does not increase for %d epochs\", not_bleu_em_inc_cnt)\n",
    "                    fa.write(\n",
    "                        \"[%d] Best bleu+em (%.2f) does not drop changed for %d epochs, cur bleu+em: %.2f (bleu: %.2f, em: %.2f)\\n\" % (\n",
    "                            cur_epoch, best_bleu_em, not_bleu_em_inc_cnt, dev_bleu_em, dev_bleu, dev_em))\n",
    "                    if all([x > args.patience for x in [not_bleu_em_inc_cnt, not_loss_dec_cnt]]):\n",
    "                        stop_early_str = \"[%d] Early stop as not_bleu_em_inc_cnt=%d, and not_loss_dec_cnt=%d\\n\" % (\n",
    "                            cur_epoch, not_bleu_em_inc_cnt, not_loss_dec_cnt)\n",
    "                        logger.info(stop_early_str)\n",
    "                        fa.write(stop_early_str)\n",
    "                        break\n",
    "        logger.info(\"***** CUDA.empty_cache() *****\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if args.local_rank in [-1, 0] and args.data_num == -1:\n",
    "        tb_writer.close()\n",
    "    logger.info(\"Finish training and take %s\", get_elapse_time(t0))\n",
    "\n",
    "if args.do_test:\n",
    "    logger.info(\"  \" + \"***** Testing *****\")\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "\n",
    "    for criteria in ['best-bleu']:\n",
    "        file = os.path.join(args.output_dir, 'checkpoint-{}/pytorch_model.bin'.format(criteria))\n",
    "        logger.info(\"Reload model from {}\".format(file))\n",
    "        model.load_state_dict(torch.load(file))\n",
    "        eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, pool, tokenizer, 'test',\n",
    "                                                           only_src=True, is_sample=False)\n",
    "        result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'test', criteria)\n",
    "        test_bleu, test_em = result['bleu'], result['em']\n",
    "        test_codebleu = result['codebleu'] if 'codebleu' in result else 0\n",
    "        result_str = \"[%s] bleu-4: %.2f, em: %.4f, codebleu: %.4f\\n\" % (criteria, test_bleu, test_em, test_codebleu)\n",
    "        logger.info(result_str)\n",
    "        fa.write(result_str)\n",
    "        if args.res_fn:\n",
    "            with open(args.res_fn, 'a+') as f:\n",
    "                f.write('[Time: {}] {}\\n'.format(get_elapse_time(t0), file))\n",
    "                f.write(result_str)\n",
    "logger.info(\"Finish and take {}\".format(get_elapse_time(t0)))\n",
    "fa.write(\"Finish and take {}\".format(get_elapse_time(t0)))\n",
    "fa.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e12647e-4b42-46fd-b27a-f640bb2537d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 16:55:29 - INFO - __main__ -     ***** Testing *****\n",
      "05/13/2024 16:55:29 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.model_type:  codet5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/13/2024 16:55:30 - INFO - models -   Finish loading model [223M] from Salesforce/codet5-base\n",
      "05/13/2024 16:55:30 - INFO - __main__ -   Reload model from sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n",
      "05/13/2024 16:55:31 - INFO - utils -   Read 1000 examples, avg src len: 14, avg trg len: 13, max src len: 94, max trg len: 98\n",
      "05/13/2024 16:55:31 - INFO - utils -   Load cache data from sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/cache_data/test_src_all.pt\n",
      "05/13/2024 16:55:31 - INFO - __main__ -     ***** Running bleu evaluation on test data*****\n",
      "05/13/2024 16:55:31 - INFO - __main__ -     Num examples = 1000\n",
      "05/13/2024 16:55:31 - INFO - __main__ -     Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval bleu for test set: 100%|███████████████████████████████████████████████████████████████████████████████████████| 63/63 [04:51<00:00,  4.63s/it]\n",
      "05/13/2024 17:00:32 - INFO - __main__ -   ***** Eval results *****\n",
      "05/13/2024 17:00:32 - INFO - __main__ -     bleu = 79.08\n",
      "05/13/2024 17:00:32 - INFO - __main__ -     codebleu = 84.8959\n",
      "05/13/2024 17:00:32 - INFO - __main__ -     em = 66.6\n",
      "05/13/2024 17:00:32 - INFO - __main__ -   [best-bleu] bleu-4: 79.08, em: 66.6000, codebleu: 84.8959\n",
      "\n",
      "05/13/2024 17:00:32 - INFO - __main__ -   Finish and take 4h14m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram match: 0.7907789938454809, weighted ngram match: 0.799421932584435, syntax_match: 0.9048335527553132, dataflow_match: 0.9007996001999\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "logger.info(\"  \" + \"***** Testing *****\")\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "config, model, tokenizer = build_or_load_gen_model(args)\n",
    "model.to(args.device)\n",
    "\n",
    "for criteria in ['best-bleu']:\n",
    "    file = 'sh/saved_models/translate/cs-java/codet5_base_all_lr5e-05_bs16_src320_trg256_pat5_e100/checkpoint-best-bleu/pytorch_model.bin' # os.path.join(args.output_dir, 'checkpoint-{}/pytorch_model.bin'.format(criteria))\n",
    "    print(file)\n",
    "    logger.info(\"Reload model from {}\".format(file))\n",
    "    model.load_state_dict(torch.load(file))\n",
    "    eval_examples, eval_data = load_and_cache_gen_data(args, args.test_filename, pool, tokenizer, 'test',\n",
    "                                                       only_src=True, is_sample=False)\n",
    "    result = eval_bleu_epoch(args, eval_data, eval_examples, model, tokenizer, 'test', criteria)\n",
    "    test_bleu, test_em = result['bleu'], result['em']\n",
    "    test_codebleu = result['codebleu'] if 'codebleu' in result else 0\n",
    "    result_str = \"[%s] bleu-4: %.2f, em: %.4f, codebleu: %.4f\\n\" % (criteria, test_bleu, test_em, test_codebleu)\n",
    "    logger.info(result_str)\n",
    "    \n",
    "logger.info(\"Finish and take {}\".format(get_elapse_time(t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a820682-a111-4a5d-90d7-baae2d349b42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
